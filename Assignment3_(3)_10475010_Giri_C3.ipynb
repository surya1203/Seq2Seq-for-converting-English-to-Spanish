{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7t3KSmW3BtYF"
   },
   "source": [
    "# Assignment 3: Build a seq2seq model for machine translation.\n",
    "\n",
    "### Name: [Surya Giri]\n",
    "\n",
    "### Task: Change LSTM model to Bidirectional LSTM Modelï¼Œ translate English to target language and evaluate using Bleu score.\n",
    "\n",
    "### Due Date: Tuesday, April 19th, 11:59PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfIJxPE7BtYK"
   },
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read and run the code. Please make sure you have installed keras or tensorflow.Running the script on colab will speed up the training process and also prevent package loading issue. \n",
    "2. Complete the code in Section 1.1, you may fill in your data directory.\n",
    "3. Directly modify the code in Section 3. Change the current LSTM layer to a Bidirectional LSTM Model.\n",
    "4. Training your model and translate English to Spanish in Section 4.2. You could try translating other languages.\n",
    "5. Complete the code in Section 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cu-6yZAnBtYM"
   },
   "source": [
    "### Hint: \n",
    "\n",
    "To implement ```Bi-LSTM```, you will need the following code to build the encoder. Do NOT use Bi-LSTM for the decoder. But there are other codes you need to modify to make it work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "g11DxuE3BtYO"
   },
   "outputs": [],
   "source": [
    "# from keras.layers import Bidirectional, Concatenate, LSTM\n",
    "\n",
    "# encoder_bilstm = Bidirectional(LSTM(latent_dim, return_state=True, \n",
    "#                                   dropout=0.5, name='encoder_lstm'))\n",
    "# _, forward_h, forward_c, backward_h, backward_c = encoder_bilstm(encoder_inputs)\n",
    "\n",
    "# state_h = Concatenate()([forward_h, backward_h])\n",
    "# state_c = Concatenate()([forward_c, backward_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "G8TNN5F3BtYQ"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Bidirectional, Concatenate, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtpA4i5QBtYS"
   },
   "source": [
    "## 1. Data preparation (10 points)\n",
    "\n",
    "1. Download spanish-english data from http://www.manythings.org/anki/\n",
    "2. You may try to use other languages.\n",
    "3. Unzip the .ZIP file.\n",
    "4. Put the .TXT file (e.g., \"deu.txt\") in the directory \"./Data/\".\n",
    "5. Fill in your data directory in section 1.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_lEEewDBtYT"
   },
   "source": [
    "### 1.1. Load and clean text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-4frYqdxBtYV"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from unicodedata import normalize\n",
    "import numpy\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs\n",
    "\n",
    "def clean_data(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return numpy.array(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvJl7TyNBtYX"
   },
   "source": [
    "#### Fill the following blanks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZO3x2p-DBtYZ"
   },
   "outputs": [],
   "source": [
    "# e.g., filename = 'Data/deu.txt'\n",
    "filename = 'spa.txt'\n",
    "\n",
    "# e.g., n_train = 20000\n",
    "n_train = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3GnWr0lcBtYa"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "doc = load_doc(filename)\n",
    "\n",
    "# split into Language1-Language2 pairs\n",
    "pairs = to_pairs(doc)\n",
    "\n",
    "# clean sentences\n",
    "clean_pairs = clean_data(pairs)[0:n_train, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ebNQpdOBBtYb",
    "outputId": "094c9d0a-370c-46f3-ce31-9225d678fb94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youre here] => [estas aqui]\n",
      "[youre here] => [estais aqui]\n",
      "[youre late] => [estas retrasado]\n",
      "[youre lost] => [estas perdido]\n",
      "[youre mean] => [eres mala]\n",
      "[youre mean] => [eres mezquino]\n",
      "[youre mine] => [tu eres mio]\n",
      "[youre nice] => [eres simpatico]\n",
      "[youre nuts] => [estas loco]\n",
      "[youre nuts] => [estas chiflado]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3000, 3010):\n",
    "    print('[' + clean_pairs[i, 0] + '] => [' + clean_pairs[i, 1] + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iHnyVpzbBtYd",
    "outputId": "ae882ee4-78b1-4b84-c560-29581bcb3358"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input_texts:  (20000,)\n",
      "Length of target_texts: (20000,)\n"
     ]
    }
   ],
   "source": [
    "input_texts = clean_pairs[:, 0]\n",
    "target_texts = ['\\t' + text + '\\n' for text in clean_pairs[:, 1]]\n",
    "\n",
    "print('Length of input_texts:  ' + str(input_texts.shape))\n",
    "print('Length of target_texts: ' + str(input_texts.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ifpfjGfvBtYe",
    "outputId": "8da9a7d6-15cb-46d6-b98c-0dd87c715901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of input  sentences: 18\n",
      "max length of target sentences: 55\n"
     ]
    }
   ],
   "source": [
    "max_encoder_seq_length = max(len(line) for line in input_texts)\n",
    "max_decoder_seq_length = max(len(line) for line in target_texts)\n",
    "\n",
    "print('max length of input  sentences: %d' % (max_encoder_seq_length))\n",
    "print('max length of target sentences: %d' % (max_decoder_seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sN6GYIJDBtYf"
   },
   "source": [
    "**Remark:** To this end, you have two lists of sentences: input_texts and target_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wlf7c1YkBtYg"
   },
   "source": [
    "## 2. Text processing\n",
    "\n",
    "### 2.1. Convert texts to sequences\n",
    "\n",
    "- Input: A list of $n$ sentences (with max length $t$).\n",
    "- It is represented by a $n\\times t$ matrix after the tokenization and zero-padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m_FIuiroBtYh",
    "outputId": "cd725277-05fd-471d-acd8-b2d0098dfee9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoder_input_seq: (20000, 18)\n",
      "shape of input_token_index: 27\n",
      "shape of decoder_input_seq: (20000, 55)\n",
      "shape of target_token_index: 29\n"
     ]
    }
   ],
   "source": [
    "# encode and pad sequences\n",
    "def text2sequences(max_len, lines):\n",
    "    tokenizer = Tokenizer(char_level=True, filters='')\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    seqs = tokenizer.texts_to_sequences(lines)\n",
    "    seqs_pad = pad_sequences(seqs, maxlen=max_len, padding='post')\n",
    "    return seqs_pad, tokenizer.word_index\n",
    "\n",
    "\n",
    "encoder_input_seq, input_token_index = text2sequences(max_encoder_seq_length, \n",
    "                                                      input_texts)\n",
    "decoder_input_seq, target_token_index = text2sequences(max_decoder_seq_length, \n",
    "                                                       target_texts)\n",
    "\n",
    "print('shape of encoder_input_seq: ' + str(encoder_input_seq.shape))\n",
    "print('shape of input_token_index: ' + str(len(input_token_index)))\n",
    "print('shape of decoder_input_seq: ' + str(decoder_input_seq.shape))\n",
    "print('shape of target_token_index: ' + str(len(target_token_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1mRWXdVBtYh",
    "outputId": "a6a522fa-9ee1-435b-b8b4-109cbe840d3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_encoder_tokens: 28\n",
      "num_decoder_tokens: 30\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens = len(input_token_index) + 1\n",
    "num_decoder_tokens = len(target_token_index) + 1\n",
    "\n",
    "print('num_encoder_tokens: ' + str(num_encoder_tokens))\n",
    "print('num_decoder_tokens: ' + str(num_decoder_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2UHH9qpBtYi"
   },
   "source": [
    "**Remark:** To this end, the input language and target language texts are converted to 2 matrices. \n",
    "\n",
    "- Their number of rows are both n_train.\n",
    "- Their number of columns are respective max_encoder_seq_length and max_decoder_seq_length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lj_mJt2BtYj"
   },
   "source": [
    "The followings print a sentence and its representation as a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "A_DqxAv1BtYj",
    "outputId": "5017af80-36e2-40cb-d7b5-2008724019cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\tno puede ser\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1sjFm1VxBtYk",
    "outputId": "94d2ee04-90e6-41b9-cce2-d9b2ace30ec4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  8,  3,  1, 17, 14,  2, 15,  2,  1,  5,  2, 10,  7,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_seq[100, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7S7n30e_BtYl"
   },
   "source": [
    "## 2.2. One-hot encode\n",
    "\n",
    "- Input: A list of $n$ sentences (with max length $t$).\n",
    "- It is represented by a $n\\times t$ matrix after the tokenization and zero-padding.\n",
    "- It is represented by a $n\\times t \\times v$ tensor ($t$ is the number of unique chars) after the one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6VydOv2OBtYm",
    "outputId": "37939aab-5d33-46b0-d65e-ae634dc1c97f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 18, 28)\n",
      "(20000, 55, 30)\n"
     ]
    }
   ],
   "source": [
    "# one hot encode target sequence\n",
    "def onehot_encode(sequences, max_len, vocab_size):\n",
    "    n = len(sequences)\n",
    "    data = numpy.zeros((n, max_len, vocab_size))\n",
    "    for i in range(n):\n",
    "        data[i, :, :] = to_categorical(sequences[i], num_classes=vocab_size)\n",
    "    return data\n",
    "\n",
    "encoder_input_data = onehot_encode(encoder_input_seq, max_encoder_seq_length, num_encoder_tokens)\n",
    "decoder_input_data = onehot_encode(decoder_input_seq, max_decoder_seq_length, num_decoder_tokens)\n",
    "\n",
    "decoder_target_seq = numpy.zeros(decoder_input_seq.shape)\n",
    "decoder_target_seq[:, 0:-1] = decoder_input_seq[:, 1:]\n",
    "decoder_target_data = onehot_encode(decoder_target_seq, \n",
    "                                    max_decoder_seq_length, \n",
    "                                    num_decoder_tokens)\n",
    "\n",
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tidjCI-JBtYm"
   },
   "source": [
    "## 3. Build the networks (for training) (20 points)\n",
    "\n",
    "- In this section, we have already implemented the LSTM model for you. You can run the code and see what the code is doing.  \n",
    "\n",
    "- You need to change the existing LSTM model to a Bidirectional LSTM model. Just modify the network structrue and do not change the training cell in section 3.4.\n",
    "\n",
    "- Build encoder, decoder, and connect the two modules to get \"model\". \n",
    "\n",
    "- Fit the model on the bilingual data to train the parameters in the encoder and decoder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DjWWirgBtYn"
   },
   "source": [
    "### 3.1. Encoder network\n",
    "\n",
    "- Input:  one-hot encode of the input language\n",
    "\n",
    "- Return: \n",
    "\n",
    "    -- output (all the hidden states   $h_1, \\cdots , h_t$) are always discarded\n",
    "    \n",
    "    -- the final hidden state  $h_t$\n",
    "    \n",
    "    -- the final conveyor belt $c_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2ZHDohZgBtYn"
   },
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "# inputs of the encoder network\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens), \n",
    "                       name='encoder_inputs')\n",
    "\n",
    "# set the LSTM layer\n",
    "encoder_lstm = Bidirectional(LSTM(latent_dim , return_state=True, \n",
    "                    dropout=0.5, name='encoder_lstm'))\n",
    "_, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "# Build the encoder network model\n",
    "encoder_model = Model(inputs=encoder_inputs,\n",
    "                        outputs=[state_h, state_c],\n",
    "                        name='encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrUTP7YgBtYo"
   },
   "source": [
    "Print a summary and save the encoder network structure to \"./encoder.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bUJFppeSBtYo",
    "outputId": "58f9a499-d500-49a7-db27-7e57fdeda92a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, None, 28)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   [(None, 256), (None, 160768      encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           bidirectional[0][1]              \n",
      "                                                                 bidirectional[0][3]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           bidirectional[0][2]              \n",
      "                                                                 bidirectional[0][4]              \n",
      "==================================================================================================\n",
      "Total params: 160,768\n",
      "Trainable params: 160,768\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "SVG(model_to_dot(encoder_model, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model(\n",
    "    model=encoder_model, show_shapes=False,\n",
    "    to_file='encoder.pdf'\n",
    ")\n",
    "\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5V5Y1wWsBtYp"
   },
   "source": [
    "### 3.2. Decoder network\n",
    "\n",
    "- Inputs:  \n",
    "\n",
    "    -- one-hot encode of the target language\n",
    "    \n",
    "    -- The initial hidden state $h_t$ \n",
    "    \n",
    "    -- The initial conveyor belt $c_t$ \n",
    "\n",
    "- Return: \n",
    "\n",
    "    -- output (all the hidden states) $h_1, \\cdots , h_t$\n",
    "\n",
    "    -- the final hidden state  $h_t$ (discarded in the training and used in the prediction)\n",
    "    \n",
    "    -- the final conveyor belt $c_t$ (discarded in the training and used in the prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "B8t78q-1BtYq"
   },
   "outputs": [],
   "source": [
    "latent_dim = latent_dim * 2\n",
    "\n",
    "# inputs of the decoder network\n",
    "decoder_input_h = Input(shape=(latent_dim,), name='decoder_input_h')\n",
    "decoder_input_c = Input(shape=(latent_dim,), name='decoder_input_c')\n",
    "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
    "\n",
    "# set the LSTM layer\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, \n",
    "                    return_state=True, dropout=0.5, name='decoder_lstm')\n",
    "decoder_lstm_outputs, state_h, state_c = decoder_lstm(decoder_input_x, \n",
    "                                                      initial_state=[decoder_input_h, decoder_input_c])\n",
    "\n",
    "# set the dense layer\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
    "\n",
    "# build the decoder network model\n",
    "decoder_model = Model(inputs=[decoder_input_x, decoder_input_h, decoder_input_c],\n",
    "                      outputs=[decoder_outputs, state_h, state_c],\n",
    "                      name='decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZFYvp-8BtYq"
   },
   "source": [
    "Print a summary and save the encoder network structure to \"./decoder.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XxH6206tBtYr",
    "outputId": "3e967d12-02b0-4b31-9234-e00d9fd56d14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input_x (InputLayer)    [(None, None, 30)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_h (InputLayer)    [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_c (InputLayer)    [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 256),  293888      decoder_input_x[0][0]            \n",
      "                                                                 decoder_input_h[0][0]            \n",
      "                                                                 decoder_input_c[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 30)     7710        decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 301,598\n",
      "Trainable params: 301,598\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "SVG(model_to_dot(decoder_model, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model(\n",
    "    model=decoder_model, show_shapes=False,\n",
    "    to_file='decoder.pdf'\n",
    ")\n",
    "\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zliNBn7KBtYs"
   },
   "source": [
    "### 3.3. Connect the encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0chVSp0WBtYs"
   },
   "outputs": [],
   "source": [
    "# input layers\n",
    "encoder_input_x = Input(shape=(None, num_encoder_tokens), name='encoder_input_x')\n",
    "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
    "\n",
    "# connect encoder to decoder\n",
    "encoder_final_states = encoder_model([encoder_input_x])\n",
    "decoder_lstm_output, _, _ = decoder_lstm(decoder_input_x, initial_state=encoder_final_states)\n",
    "decoder_pred = decoder_dense(decoder_lstm_output)\n",
    "\n",
    "model = Model(inputs=[encoder_input_x, decoder_input_x], \n",
    "              outputs=decoder_pred, \n",
    "              name='model_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96cWXGvvBtYt",
    "outputId": "1eb3d197-3ed1-4a61-d62c-f35feedde286"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_training\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input_x (InputLayer)    [(None, None, 28)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_x (InputLayer)    [(None, None, 30)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            [(None, 256), (None, 160768      encoder_input_x[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 256),  293888      decoder_input_x[0][0]            \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 encoder[0][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 30)     7710        decoder_lstm[1][0]               \n",
      "==================================================================================================\n",
      "Total params: 462,366\n",
      "Trainable params: 462,366\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model(\n",
    "    model=model, show_shapes=False,\n",
    "    to_file='model_training.pdf'\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k20KxtNABtYu"
   },
   "source": [
    "### 3.4. Fit the model on the bilingual dataset\n",
    "\n",
    "- encoder_input_data: one-hot encode of the input language\n",
    "\n",
    "- decoder_input_data: one-hot encode of the input language\n",
    "\n",
    "- decoder_target_data: labels (left shift of decoder_input_data)\n",
    "\n",
    "- tune the hyper-parameters\n",
    "\n",
    "- stop when the validation loss stop decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SAjNtIFIBtYu",
    "outputId": "3f92acf8-4f41-456a-d57a-0894f781f685"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoder_input_data(20000, 18, 28)\n",
      "shape of decoder_input_data(20000, 55, 30)\n",
      "shape of decoder_target_data(20000, 55, 30)\n"
     ]
    }
   ],
   "source": [
    "print('shape of encoder_input_data' + str(encoder_input_data.shape))\n",
    "print('shape of decoder_input_data' + str(decoder_input_data.shape))\n",
    "print('shape of decoder_target_data' + str(decoder_target_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping to prevent over-training \n",
    "callback = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "\n",
    "# # Save the best model with lowest val_loss\n",
    "# checkpoint = ModelCheckpoint(filepath=\"best1.hdf5\", verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UYsmvHwBtYv",
    "outputId": "8ab7a74c-a26b-4ff5-8d1f-384058cdfd70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1000/1000 [==============================] - 14s 10ms/step - loss: 0.9021 - accuracy: 0.7451 - val_loss: 0.9211 - val_accuracy: 0.7282\n",
      "Epoch 2/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.7570 - accuracy: 0.7737 - val_loss: 0.7971 - val_accuracy: 0.7603\n",
      "Epoch 3/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.7228 - accuracy: 0.7805 - val_loss: 0.7584 - val_accuracy: 0.7671\n",
      "Epoch 4/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.7036 - accuracy: 0.7853 - val_loss: 0.7307 - val_accuracy: 0.7751\n",
      "Epoch 5/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.6877 - accuracy: 0.7904 - val_loss: 0.7163 - val_accuracy: 0.7784\n",
      "Epoch 6/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.6744 - accuracy: 0.7945 - val_loss: 0.6917 - val_accuracy: 0.7862\n",
      "Epoch 7/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.6623 - accuracy: 0.7983 - val_loss: 0.6752 - val_accuracy: 0.7923\n",
      "Epoch 8/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.6529 - accuracy: 0.8011 - val_loss: 0.6608 - val_accuracy: 0.7945\n",
      "Epoch 9/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.6437 - accuracy: 0.8039 - val_loss: 0.6505 - val_accuracy: 0.7975\n",
      "Epoch 10/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.6359 - accuracy: 0.8063 - val_loss: 0.6393 - val_accuracy: 0.8018\n",
      "Epoch 11/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.6284 - accuracy: 0.8089 - val_loss: 0.6257 - val_accuracy: 0.8064\n",
      "Epoch 12/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.6226 - accuracy: 0.8105 - val_loss: 0.6179 - val_accuracy: 0.8076\n",
      "Epoch 13/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.6157 - accuracy: 0.8127 - val_loss: 0.6100 - val_accuracy: 0.8102\n",
      "Epoch 14/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.6090 - accuracy: 0.8145 - val_loss: 0.6080 - val_accuracy: 0.8117\n",
      "Epoch 15/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.6040 - accuracy: 0.8165 - val_loss: 0.5958 - val_accuracy: 0.8147\n",
      "Epoch 16/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5994 - accuracy: 0.8175 - val_loss: 0.5887 - val_accuracy: 0.8170\n",
      "Epoch 17/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5944 - accuracy: 0.8190 - val_loss: 0.5855 - val_accuracy: 0.8183\n",
      "Epoch 18/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5892 - accuracy: 0.8205 - val_loss: 0.5794 - val_accuracy: 0.8203\n",
      "Epoch 19/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5846 - accuracy: 0.8222 - val_loss: 0.5717 - val_accuracy: 0.8226\n",
      "Epoch 20/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5828 - accuracy: 0.8228 - val_loss: 0.5677 - val_accuracy: 0.8242\n",
      "Epoch 21/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5764 - accuracy: 0.8248 - val_loss: 0.5610 - val_accuracy: 0.8259\n",
      "Epoch 22/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5720 - accuracy: 0.8261 - val_loss: 0.5609 - val_accuracy: 0.8261\n",
      "Epoch 23/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5702 - accuracy: 0.8267 - val_loss: 0.5533 - val_accuracy: 0.8280\n",
      "Epoch 24/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5664 - accuracy: 0.8276 - val_loss: 0.5502 - val_accuracy: 0.8289\n",
      "Epoch 25/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5633 - accuracy: 0.8287 - val_loss: 0.5450 - val_accuracy: 0.8313\n",
      "Epoch 26/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5578 - accuracy: 0.8305 - val_loss: 0.5440 - val_accuracy: 0.8314\n",
      "Epoch 27/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5561 - accuracy: 0.8309 - val_loss: 0.5372 - val_accuracy: 0.8331\n",
      "Epoch 28/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5536 - accuracy: 0.8319 - val_loss: 0.5370 - val_accuracy: 0.8332\n",
      "Epoch 29/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5506 - accuracy: 0.8328 - val_loss: 0.5324 - val_accuracy: 0.8346\n",
      "Epoch 30/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5461 - accuracy: 0.8342 - val_loss: 0.5305 - val_accuracy: 0.8353\n",
      "Epoch 31/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5455 - accuracy: 0.8343 - val_loss: 0.5277 - val_accuracy: 0.8356\n",
      "Epoch 32/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5430 - accuracy: 0.8351 - val_loss: 0.5231 - val_accuracy: 0.8374\n",
      "Epoch 33/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5390 - accuracy: 0.8359 - val_loss: 0.5212 - val_accuracy: 0.8383\n",
      "Epoch 34/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5370 - accuracy: 0.8369 - val_loss: 0.5184 - val_accuracy: 0.8391\n",
      "Epoch 35/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5361 - accuracy: 0.8374 - val_loss: 0.5172 - val_accuracy: 0.8386\n",
      "Epoch 36/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5334 - accuracy: 0.8382 - val_loss: 0.5156 - val_accuracy: 0.8400\n",
      "Epoch 37/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5300 - accuracy: 0.8389 - val_loss: 0.5123 - val_accuracy: 0.8414\n",
      "Epoch 38/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5283 - accuracy: 0.8395 - val_loss: 0.5102 - val_accuracy: 0.8418\n",
      "Epoch 39/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5270 - accuracy: 0.8400 - val_loss: 0.5087 - val_accuracy: 0.8415\n",
      "Epoch 40/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5248 - accuracy: 0.8404 - val_loss: 0.5064 - val_accuracy: 0.8430\n",
      "Epoch 41/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5209 - accuracy: 0.8416 - val_loss: 0.5051 - val_accuracy: 0.8433\n",
      "Epoch 42/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5198 - accuracy: 0.8422 - val_loss: 0.5023 - val_accuracy: 0.8444\n",
      "Epoch 43/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.5180 - accuracy: 0.8425 - val_loss: 0.4994 - val_accuracy: 0.8456\n",
      "Epoch 44/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5163 - accuracy: 0.8429 - val_loss: 0.4994 - val_accuracy: 0.8454\n",
      "Epoch 45/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.5139 - accuracy: 0.8437 - val_loss: 0.4980 - val_accuracy: 0.8462\n",
      "Epoch 46/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.5117 - accuracy: 0.8445 - val_loss: 0.4952 - val_accuracy: 0.8472\n",
      "Epoch 47/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.5116 - accuracy: 0.8444 - val_loss: 0.4930 - val_accuracy: 0.8479\n",
      "Epoch 48/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.5095 - accuracy: 0.8452 - val_loss: 0.4939 - val_accuracy: 0.8469\n",
      "Epoch 49/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.5078 - accuracy: 0.8454 - val_loss: 0.4912 - val_accuracy: 0.8479\n",
      "Epoch 50/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5066 - accuracy: 0.8459 - val_loss: 0.4899 - val_accuracy: 0.8486\n",
      "Epoch 51/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5044 - accuracy: 0.8469 - val_loss: 0.4881 - val_accuracy: 0.8492\n",
      "Epoch 52/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5030 - accuracy: 0.8471 - val_loss: 0.4898 - val_accuracy: 0.8487\n",
      "Epoch 53/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5022 - accuracy: 0.8471 - val_loss: 0.4884 - val_accuracy: 0.8491\n",
      "Epoch 54/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5003 - accuracy: 0.8481 - val_loss: 0.4837 - val_accuracy: 0.8508\n",
      "Epoch 55/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4998 - accuracy: 0.8484 - val_loss: 0.4858 - val_accuracy: 0.8498\n",
      "Epoch 56/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4981 - accuracy: 0.8484 - val_loss: 0.4823 - val_accuracy: 0.8510\n",
      "Epoch 57/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4967 - accuracy: 0.8487 - val_loss: 0.4817 - val_accuracy: 0.8508\n",
      "Epoch 58/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4946 - accuracy: 0.8493 - val_loss: 0.4800 - val_accuracy: 0.8516\n",
      "Epoch 59/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4938 - accuracy: 0.8498 - val_loss: 0.4803 - val_accuracy: 0.8515\n",
      "Epoch 60/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4913 - accuracy: 0.8509 - val_loss: 0.4817 - val_accuracy: 0.8519\n",
      "Epoch 61/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4917 - accuracy: 0.8507 - val_loss: 0.4775 - val_accuracy: 0.8530\n",
      "Epoch 62/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4894 - accuracy: 0.8511 - val_loss: 0.4765 - val_accuracy: 0.8532\n",
      "Epoch 63/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4892 - accuracy: 0.8511 - val_loss: 0.4773 - val_accuracy: 0.8525\n",
      "Epoch 64/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4877 - accuracy: 0.8514 - val_loss: 0.4743 - val_accuracy: 0.8537\n",
      "Epoch 65/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4868 - accuracy: 0.8517 - val_loss: 0.4750 - val_accuracy: 0.8535\n",
      "Epoch 66/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4852 - accuracy: 0.8522 - val_loss: 0.4716 - val_accuracy: 0.8544\n",
      "Epoch 67/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4850 - accuracy: 0.8525 - val_loss: 0.4755 - val_accuracy: 0.8533\n",
      "Epoch 68/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4819 - accuracy: 0.8534 - val_loss: 0.4732 - val_accuracy: 0.8538\n",
      "Epoch 69/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4824 - accuracy: 0.8530 - val_loss: 0.4721 - val_accuracy: 0.8540\n",
      "Epoch 70/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4795 - accuracy: 0.8541 - val_loss: 0.4703 - val_accuracy: 0.8552\n",
      "Epoch 71/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4796 - accuracy: 0.8541 - val_loss: 0.4701 - val_accuracy: 0.8552\n",
      "Epoch 72/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4794 - accuracy: 0.8540 - val_loss: 0.4679 - val_accuracy: 0.8559\n",
      "Epoch 73/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4782 - accuracy: 0.8543 - val_loss: 0.4688 - val_accuracy: 0.8555\n",
      "Epoch 74/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4757 - accuracy: 0.8550 - val_loss: 0.4676 - val_accuracy: 0.8560\n",
      "Epoch 75/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4758 - accuracy: 0.8551 - val_loss: 0.4644 - val_accuracy: 0.8574\n",
      "Epoch 76/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4737 - accuracy: 0.8558 - val_loss: 0.4686 - val_accuracy: 0.8562\n",
      "Epoch 77/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4753 - accuracy: 0.8553 - val_loss: 0.4658 - val_accuracy: 0.8567\n",
      "Epoch 78/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4736 - accuracy: 0.8558 - val_loss: 0.4650 - val_accuracy: 0.8566\n",
      "Epoch 79/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4722 - accuracy: 0.8562 - val_loss: 0.4642 - val_accuracy: 0.8571\n",
      "Epoch 80/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4708 - accuracy: 0.8567 - val_loss: 0.4629 - val_accuracy: 0.8573\n",
      "Epoch 81/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4686 - accuracy: 0.8573 - val_loss: 0.4616 - val_accuracy: 0.8577\n",
      "Epoch 82/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4677 - accuracy: 0.8573 - val_loss: 0.4616 - val_accuracy: 0.8580\n",
      "Epoch 83/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4682 - accuracy: 0.8571 - val_loss: 0.4613 - val_accuracy: 0.8581\n",
      "Epoch 84/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4668 - accuracy: 0.8579 - val_loss: 0.4609 - val_accuracy: 0.8579\n",
      "Epoch 85/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4654 - accuracy: 0.8582 - val_loss: 0.4613 - val_accuracy: 0.8579\n",
      "Epoch 86/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4633 - accuracy: 0.8590 - val_loss: 0.4573 - val_accuracy: 0.8598\n",
      "Epoch 87/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4635 - accuracy: 0.8586 - val_loss: 0.4616 - val_accuracy: 0.8580\n",
      "Epoch 88/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4633 - accuracy: 0.8584 - val_loss: 0.4579 - val_accuracy: 0.8588\n",
      "Epoch 89/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4623 - accuracy: 0.8592 - val_loss: 0.4589 - val_accuracy: 0.8590\n",
      "Epoch 90/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4632 - accuracy: 0.8589 - val_loss: 0.4585 - val_accuracy: 0.8589\n",
      "Epoch 91/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4595 - accuracy: 0.8597 - val_loss: 0.4620 - val_accuracy: 0.8581\n",
      "Epoch 92/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4606 - accuracy: 0.8594 - val_loss: 0.4603 - val_accuracy: 0.8591\n",
      "Epoch 93/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4609 - accuracy: 0.8591 - val_loss: 0.4582 - val_accuracy: 0.8587\n",
      "Epoch 94/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4589 - accuracy: 0.8602 - val_loss: 0.4587 - val_accuracy: 0.8593\n",
      "Epoch 95/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4583 - accuracy: 0.8604 - val_loss: 0.4549 - val_accuracy: 0.8606\n",
      "Epoch 96/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4569 - accuracy: 0.8605 - val_loss: 0.4540 - val_accuracy: 0.8610\n",
      "Epoch 97/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4572 - accuracy: 0.8608 - val_loss: 0.4549 - val_accuracy: 0.8605\n",
      "Epoch 98/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4576 - accuracy: 0.8605 - val_loss: 0.4564 - val_accuracy: 0.8601\n",
      "Epoch 99/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4563 - accuracy: 0.8610 - val_loss: 0.4549 - val_accuracy: 0.8605\n",
      "Epoch 100/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4545 - accuracy: 0.8609 - val_loss: 0.4531 - val_accuracy: 0.8608\n",
      "Epoch 101/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4534 - accuracy: 0.8616 - val_loss: 0.4544 - val_accuracy: 0.8607\n",
      "Epoch 102/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4545 - accuracy: 0.8614 - val_loss: 0.4558 - val_accuracy: 0.8601\n",
      "Epoch 103/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4530 - accuracy: 0.8618 - val_loss: 0.4526 - val_accuracy: 0.8609\n",
      "Epoch 104/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4520 - accuracy: 0.8621 - val_loss: 0.4503 - val_accuracy: 0.8619\n",
      "Epoch 105/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4509 - accuracy: 0.8625 - val_loss: 0.4547 - val_accuracy: 0.8604\n",
      "Epoch 106/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4520 - accuracy: 0.8622 - val_loss: 0.4528 - val_accuracy: 0.8610\n",
      "Epoch 107/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4491 - accuracy: 0.8631 - val_loss: 0.4507 - val_accuracy: 0.8616\n",
      "Epoch 108/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4474 - accuracy: 0.8636 - val_loss: 0.4526 - val_accuracy: 0.8609\n",
      "Epoch 109/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4489 - accuracy: 0.8625 - val_loss: 0.4515 - val_accuracy: 0.8613\n",
      "Epoch 110/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4490 - accuracy: 0.8629 - val_loss: 0.4516 - val_accuracy: 0.8619\n",
      "Epoch 111/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4484 - accuracy: 0.8629 - val_loss: 0.4504 - val_accuracy: 0.8625\n",
      "Epoch 112/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4466 - accuracy: 0.8637 - val_loss: 0.4522 - val_accuracy: 0.8614\n",
      "Epoch 113/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4477 - accuracy: 0.8629 - val_loss: 0.4534 - val_accuracy: 0.8609\n",
      "Epoch 114/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4465 - accuracy: 0.8638 - val_loss: 0.4520 - val_accuracy: 0.8611\n",
      "Epoch 115/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4465 - accuracy: 0.8637 - val_loss: 0.4502 - val_accuracy: 0.8617\n",
      "Epoch 116/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4434 - accuracy: 0.8648 - val_loss: 0.4483 - val_accuracy: 0.8628\n",
      "Epoch 117/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4444 - accuracy: 0.8643 - val_loss: 0.4508 - val_accuracy: 0.8617\n",
      "Epoch 118/150\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4427 - accuracy: 0.8649 - val_loss: 0.4475 - val_accuracy: 0.8627\n",
      "Epoch 119/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4420 - accuracy: 0.8648 - val_loss: 0.4489 - val_accuracy: 0.8627\n",
      "Epoch 120/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4413 - accuracy: 0.8654 - val_loss: 0.4513 - val_accuracy: 0.8620\n",
      "Epoch 121/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4402 - accuracy: 0.8655 - val_loss: 0.4479 - val_accuracy: 0.8630\n",
      "Epoch 122/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4397 - accuracy: 0.8659 - val_loss: 0.4506 - val_accuracy: 0.8620\n",
      "Epoch 123/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4393 - accuracy: 0.8660 - val_loss: 0.4483 - val_accuracy: 0.8630\n",
      "Epoch 124/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4399 - accuracy: 0.8659 - val_loss: 0.4458 - val_accuracy: 0.8633\n",
      "Epoch 125/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4407 - accuracy: 0.8654 - val_loss: 0.4482 - val_accuracy: 0.8622\n",
      "Epoch 126/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4372 - accuracy: 0.8661 - val_loss: 0.4486 - val_accuracy: 0.8623\n",
      "Epoch 127/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4380 - accuracy: 0.8664 - val_loss: 0.4473 - val_accuracy: 0.8634\n",
      "Epoch 128/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4360 - accuracy: 0.8669 - val_loss: 0.4470 - val_accuracy: 0.8633\n",
      "Epoch 129/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4379 - accuracy: 0.8663 - val_loss: 0.4500 - val_accuracy: 0.8621\n",
      "Epoch 130/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4346 - accuracy: 0.8673 - val_loss: 0.4452 - val_accuracy: 0.8641\n",
      "Epoch 131/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4363 - accuracy: 0.8670 - val_loss: 0.4461 - val_accuracy: 0.8632\n",
      "Epoch 132/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4362 - accuracy: 0.8667 - val_loss: 0.4440 - val_accuracy: 0.8644\n",
      "Epoch 133/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4337 - accuracy: 0.8676 - val_loss: 0.4472 - val_accuracy: 0.8633\n",
      "Epoch 134/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4338 - accuracy: 0.8674 - val_loss: 0.4472 - val_accuracy: 0.8631\n",
      "Epoch 135/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4353 - accuracy: 0.8669 - val_loss: 0.4455 - val_accuracy: 0.8634\n",
      "Epoch 136/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4334 - accuracy: 0.8675 - val_loss: 0.4465 - val_accuracy: 0.8636\n",
      "Epoch 137/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4329 - accuracy: 0.8676 - val_loss: 0.4476 - val_accuracy: 0.8631\n",
      "Epoch 138/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4329 - accuracy: 0.8675 - val_loss: 0.4444 - val_accuracy: 0.8647\n",
      "Epoch 139/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4313 - accuracy: 0.8685 - val_loss: 0.4445 - val_accuracy: 0.8643\n",
      "Epoch 140/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4310 - accuracy: 0.8683 - val_loss: 0.4453 - val_accuracy: 0.8633\n",
      "Epoch 141/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4326 - accuracy: 0.8676 - val_loss: 0.4455 - val_accuracy: 0.8642\n",
      "Epoch 142/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4300 - accuracy: 0.8682 - val_loss: 0.4457 - val_accuracy: 0.8641\n",
      "Epoch 143/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4291 - accuracy: 0.8687 - val_loss: 0.4436 - val_accuracy: 0.8642\n",
      "Epoch 144/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4303 - accuracy: 0.8682 - val_loss: 0.4468 - val_accuracy: 0.8640\n",
      "Epoch 145/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4286 - accuracy: 0.8687 - val_loss: 0.4432 - val_accuracy: 0.8644\n",
      "Epoch 146/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4289 - accuracy: 0.8691 - val_loss: 0.4439 - val_accuracy: 0.8643\n",
      "Epoch 147/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4291 - accuracy: 0.8685 - val_loss: 0.4422 - val_accuracy: 0.8648\n",
      "Epoch 148/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4283 - accuracy: 0.8689 - val_loss: 0.4423 - val_accuracy: 0.8646\n",
      "Epoch 149/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4285 - accuracy: 0.8691 - val_loss: 0.4444 - val_accuracy: 0.8645\n",
      "Epoch 150/150\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4268 - accuracy: 0.8693 - val_loss: 0.4436 - val_accuracy: 0.8650\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adamax', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data],  # training data\n",
    "          decoder_target_data,                       # labels (left shift of the target sequences)\n",
    "          batch_size=16, epochs=150, validation_split=0.2, callbacks=[callback])\n",
    "\n",
    "model.save('seq2seq.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NzlYIO0BtYv"
   },
   "source": [
    "## 4. Make predictions\n",
    "\n",
    "- In this section, you need to complete section 4.2 to translate English to the target language.\n",
    "\n",
    "\n",
    "### 4.1. Translate English to XXX\n",
    "\n",
    "1. Encoder read a sentence (source language) and output its final states, $h_t$ and $c_t$.\n",
    "2. Take the [star] sign \"\\t\" and the final state $h_t$ and $c_t$ as input and run the decoder.\n",
    "3. Get the new states and predicted probability distribution.\n",
    "4. sample a char from the predicted probability distribution\n",
    "5. take the sampled char and the new states as input and repeat the process (stop if reach the [stop] sign \"\\n\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "UxBqbET6BtYw"
   },
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "5OmdaCvdBtYw"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, temperature = 0.2):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # this line of code is greedy selection\n",
    "        # try to use multinomial sampling instead (with temperature)\n",
    "        # sampled_token_index = numpy.argmax(output_tokens[0, -1, :])\n",
    "\n",
    "        # Multinomial sampling with temperature\n",
    "        p = output_tokens[0, -1, :]\n",
    "        p = numpy.asarray(p).astype('float64')\n",
    "        p = p ** (1 / temperature)\n",
    "        p = p / numpy.sum(p)\n",
    "\n",
    "        next_onehot = numpy.random.multinomial(1,p,1)\n",
    "        sampled_token_index = numpy.argmax(next_onehot)\n",
    "\n",
    "        # To handle the 0 index error\n",
    "        if sampled_token_index == 0:\n",
    "          break\n",
    "        \n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V9_ILHeqBtYx",
    "outputId": "edb8728c-c99e-45e3-8ff1-d639434ebc36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "English:        hes skinny\n",
      "Spanish (true):  el esta delgado\n",
      "Spanish (pred):  es esta muerto\n",
      "-\n",
      "English:        hes strong\n",
      "Spanish (true):  el es fuerte\n",
      "Spanish (pred):  el es abarrido\n",
      "-\n",
      "English:        hes stupid\n",
      "Spanish (true):  el es estupido\n",
      "Spanish (pred):  es an lesto\n",
      "-\n",
      "English:        hes stupid\n",
      "Spanish (true):  no le llega agua al tanque\n",
      "Spanish (pred):  el es a seguro\n",
      "-\n",
      "English:        hes stupid\n",
      "Spanish (true):  es un salame\n",
      "Spanish (pred):  el es antesigente\n",
      "-\n",
      "English:        help me out\n",
      "Spanish (true):  ayudame\n",
      "Spanish (pred):  ayudame a tom\n",
      "-\n",
      "English:        help me out\n",
      "Spanish (true):  ayudame a salir\n",
      "Spanish (pred):  ayudame a tom\n",
      "-\n",
      "English:        help me out\n",
      "Spanish (true):  echeme la mano\n",
      "Spanish (pred):  ayudame a tom\n",
      "-\n",
      "English:        help me out\n",
      "Spanish (true):  ayudame a salir\n",
      "Spanish (pred):  ayudame a mary\n",
      "-\n",
      "English:        here i come\n",
      "Spanish (true):  aqui vengo\n",
      "Spanish (pred):  aqui esta tom\n",
      "-\n",
      "English:        here i come\n",
      "Spanish (true):  ya estoy aqui\n",
      "Spanish (pred):  aqui esta tom\n",
      "-\n",
      "English:        here she is\n",
      "Spanish (true):  aqui esta ella\n",
      "Spanish (pred):  aqui esta eso\n",
      "-\n",
      "English:        here we are\n",
      "Spanish (true):  aqui estamos\n",
      "Spanish (pred):  aqui estamos\n",
      "-\n",
      "English:        here we are\n",
      "Spanish (true):  aqui estamos\n",
      "Spanish (pred):  aqui estamos\n",
      "-\n",
      "English:        hi im tom\n",
      "Spanish (true):  hola soy tom\n",
      "Spanish (pred):  el es tom\n",
      "-\n",
      "English:        hit it hard\n",
      "Spanish (true):  dele fuerte\n",
      "Spanish (pred):  esta a mi\n",
      "-\n",
      "English:        how are you\n",
      "Spanish (true):  como estas vos\n",
      "Spanish (pred):  como estas\n",
      "-\n",
      "English:        how curious\n",
      "Spanish (true):  que curioso\n",
      "Spanish (pred):  que tan tiento\n",
      "-\n",
      "English:        how strange\n",
      "Spanish (true):  que raro\n",
      "Spanish (pred):  que rorro\n",
      "-\n",
      "English:        how strange\n",
      "Spanish (true):  que raro\n",
      "Spanish (pred):  que tal grande\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(2100, 2120):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('English:       ', input_texts[seq_index])\n",
    "    print('Spanish (true): ', target_texts[seq_index][1:-1])\n",
    "    print('Spanish (pred): ', decoded_sentence[0:-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVWiL9YzBtYx"
   },
   "source": [
    "### 4.2. Translate an English sentence to the target language ï¼ˆ20 pointsï¼‰\n",
    "\n",
    "1. Tokenization\n",
    "2. One-hot encode\n",
    "3. Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KNy-7FuCBtYy",
    "outputId": "ea5f7c2a-d93c-450e-ec63-8dd0434fe0fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source sentence is: I love you\n",
      "translated sentence is: yo amo los mustas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_sentence = 'I love you'\n",
    "\n",
    "tokenizer = Tokenizer(char_level=True, filters='')\n",
    "\n",
    "tokenizer.word_index = input_token_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences([input_sentence])\n",
    "\n",
    "input_sequence = pad_sequences(sequences, maxlen=max_encoder_seq_length, padding='post')\n",
    "\n",
    "input_x = onehot_encode(input_sequence,max_encoder_seq_length,num_encoder_tokens )\n",
    "\n",
    "translated_sentence = decode_sequence(input_x)\n",
    "\n",
    "print('source sentence is: ' + input_sentence)\n",
    "print('translated sentence is: ' + translated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIcdJp9VBtYy"
   },
   "source": [
    "# 5. Evaluate the translation using BLEU score\n",
    "\n",
    "- We have already translated from English to target language, but how can we evaluate the performance of our model quantitatively? \n",
    "\n",
    "- In this section, you need to re-train the model we built in secton 3 and then evaluate the bleu score on testing dataset.\n",
    "\n",
    "Reference:\n",
    "\n",
    "https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
    "\n",
    "https://en.wikipedia.org/wiki/BLEU\n",
    "\n",
    "#### Hint:\n",
    "\n",
    "- Randomly partition the dataset to training, validation, and test.\n",
    "\n",
    "- Evaluate the BLEU score using the test set. Report the average.\n",
    "\n",
    "- You may use packages to calculate bleu score, e.g., sentence_bleu() from nltk package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cup67z6ABtYz"
   },
   "source": [
    "### 5.1. Partition the dataset to training, validation, and test. Build new token index. (10 points)\n",
    "\n",
    "1. You may try to load more data/lines from text file.\n",
    "2. Convert text to sequences and build token index using training data.\n",
    "3. One-hot encode your training and validation text sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "b4bLD6RsBtYz"
   },
   "outputs": [],
   "source": [
    "clean_pairs = clean_data(pairs)[0:120000, :]\n",
    "input_texts = clean_pairs[:, 0]\n",
    "target_texts = ['\\t' + text + '\\n' for text in clean_pairs[:, 1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Sk2nkm3zBtY0"
   },
   "outputs": [],
   "source": [
    "max_encoder_seq_length = max(len(line) for line in input_texts)\n",
    "max_decoder_seq_length = max(len(line) for line in target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "0w6CRY48BtY0"
   },
   "outputs": [],
   "source": [
    "encoder_input_seq, input_token_index = text2sequences(max_encoder_seq_length, \n",
    "                                                      input_texts)\n",
    "decoder_input_seq, target_token_index = text2sequences(max_decoder_seq_length, \n",
    "                                                       target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Fq7S6PknBtY0"
   },
   "outputs": [],
   "source": [
    "num_encoder_tokens = len(input_token_index) + 1\n",
    "num_decoder_tokens = len(target_token_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "amroU7XjBtY1"
   },
   "outputs": [],
   "source": [
    "encoder_input_data = onehot_encode(encoder_input_seq, max_encoder_seq_length, num_encoder_tokens)\n",
    "decoder_input_data = onehot_encode(decoder_input_seq, max_decoder_seq_length, num_decoder_tokens)\n",
    "\n",
    "decoder_target_seq = numpy.zeros(decoder_input_seq.shape)\n",
    "decoder_target_seq[:, 0:-1] = decoder_input_seq[:, 1:]\n",
    "decoder_target_data = onehot_encode(decoder_target_seq, \n",
    "                                    max_decoder_seq_length, \n",
    "                                    num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mb5HbVsKBtY1",
    "outputId": "3f270157-d4ab-4cc2-f31b-f8e619b6dfe4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 87, 30)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5A3SSRcjBtY1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5k88ifxBtY2",
    "outputId": "a6b5ed24-01ba-4a30-c8ad-4e8eb21fc781"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TQvqPyLhBtY2",
    "outputId": "ae13fadc-c977-45fc-b397-5fdaf974a726"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8YM8pPvBtY3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BbQyNgjbBtY3",
    "outputId": "6fd05e9e-b876-4f62-9a0f-1fc3e43e08b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_encoder_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RWxmJbIDBtY3",
    "outputId": "dd92a650-b494-4b59-a191-d212be843097"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_decoder_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5Hg8SLpBtY4",
    "outputId": "662824f8-e56f-4ba6-ec80-236f0589f28a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 44)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W1oq64G1BtY4",
    "outputId": "0ec7d916-603c-45ba-b1e8-2d677d24aa3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AFS3CrZVBtY5",
    "outputId": "21e5f2c1-5d1e-4caa-e9ca-36db2b260051"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 87)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2QhPkmbBtY5",
    "outputId": "d6c64bc4-c5b0-41ff-f5d7-4e61850d76be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9XJuW4C5BtY6",
    "outputId": "15f7f0d0-b2c2-403e-9d62-7b6768690a6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_encoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nYqDJfjTBtY6",
    "outputId": "10188ada-c294-4338-8661-06b307dfee15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FEmI7ntrBtY7",
    "outputId": "fd14f677-2a6c-4a3b-ab19-83d3062cbe59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 44, 28)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xRQ4bresBtY7",
    "outputId": "fcfaa670-5718-4213-a32e-d1f989d36b57"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 87, 30)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWZdORugBtY7",
    "outputId": "e0a5c64e-f937-4a3b-c3a8-b88823e9486a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 87, 30)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "MoDBNWPWBtY8"
   },
   "outputs": [],
   "source": [
    "rand_indices = numpy.random.permutation(120000)\n",
    "train_indices = rand_indices[0:96000]\n",
    "valid_indices = rand_indices[96000:108000]\n",
    "test_indices = rand_indices[108000:120000]\n",
    "\n",
    "\n",
    "encoder_input_data_train = encoder_input_data[train_indices,:]\n",
    "encoder_input_data_valid = encoder_input_data[valid_indices,:]\n",
    "encoder_input_data_test = encoder_input_data[test_indices,:] \n",
    "\n",
    "decoder_input_data_train = decoder_input_data[train_indices,:]\n",
    "decoder_input_data_valid = decoder_input_data[valid_indices,:]\n",
    "decoder_input_data_test = decoder_input_data[test_indices,:] \n",
    "\n",
    "\n",
    "decoder_target_data_train = decoder_target_data[train_indices,:]\n",
    "decoder_target_data_valid = decoder_target_data[valid_indices,:]\n",
    "decoder_target_data_test = decoder_target_data[test_indices,:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sRBPSs3BtY8"
   },
   "source": [
    "### 5.2 Retrain your previous Bidirectional LSTM model with training and validation data and tune the parameters (learning rate, optimizer, etc) based on validation score. (25 points)\n",
    "\n",
    "1. Use the model structure in section 3 to train a new model with new training and validation datasets.\n",
    "2. Based on validation BLEU score or loss to tune parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping to prevent over-training \n",
    "callback = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "\n",
    "# # Save the best model with lowest val_loss\n",
    "# checkpoint = ModelCheckpoint(filepath=\"best2.hdf5\", verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "J7bxHwZ_BtY9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 81s 13ms/step - loss: 0.7550 - accuracy: 0.7714 - val_loss: 0.5753 - val_accuracy: 0.8214\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 77s 13ms/step - loss: 0.6902 - accuracy: 0.7885 - val_loss: 0.5322 - val_accuracy: 0.8348\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.6674 - accuracy: 0.7956 - val_loss: 0.5086 - val_accuracy: 0.8423\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 77s 13ms/step - loss: 0.6519 - accuracy: 0.8004 - val_loss: 0.4899 - val_accuracy: 0.8485\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 77s 13ms/step - loss: 0.6405 - accuracy: 0.8039 - val_loss: 0.4767 - val_accuracy: 0.8525\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.6315 - accuracy: 0.8068 - val_loss: 0.4653 - val_accuracy: 0.8559\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 77s 13ms/step - loss: 0.6241 - accuracy: 0.8091 - val_loss: 0.4571 - val_accuracy: 0.8585\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.6177 - accuracy: 0.8111 - val_loss: 0.4490 - val_accuracy: 0.8609\n",
      "Epoch 9/100\n",
      "6000/6000 [==============================] - 78s 13ms/step - loss: 0.6122 - accuracy: 0.8128 - val_loss: 0.4427 - val_accuracy: 0.8630\n",
      "Epoch 10/100\n",
      "6000/6000 [==============================] - 78s 13ms/step - loss: 0.6066 - accuracy: 0.8145 - val_loss: 0.4371 - val_accuracy: 0.8648\n",
      "Epoch 11/100\n",
      "6000/6000 [==============================] - 79s 13ms/step - loss: 0.6024 - accuracy: 0.8158 - val_loss: 0.4309 - val_accuracy: 0.8666\n",
      "Epoch 12/100\n",
      "6000/6000 [==============================] - 78s 13ms/step - loss: 0.5988 - accuracy: 0.8169 - val_loss: 0.4288 - val_accuracy: 0.8669\n",
      "Epoch 13/100\n",
      "6000/6000 [==============================] - 77s 13ms/step - loss: 0.5948 - accuracy: 0.8182 - val_loss: 0.4233 - val_accuracy: 0.8692\n",
      "Epoch 14/100\n",
      "6000/6000 [==============================] - 78s 13ms/step - loss: 0.5921 - accuracy: 0.8190 - val_loss: 0.4189 - val_accuracy: 0.8705\n",
      "Epoch 15/100\n",
      "6000/6000 [==============================] - 78s 13ms/step - loss: 0.5890 - accuracy: 0.8199 - val_loss: 0.4143 - val_accuracy: 0.8718\n",
      "Epoch 16/100\n",
      "6000/6000 [==============================] - 77s 13ms/step - loss: 0.5861 - accuracy: 0.8209 - val_loss: 0.4128 - val_accuracy: 0.8724\n",
      "Epoch 17/100\n",
      "6000/6000 [==============================] - 77s 13ms/step - loss: 0.5835 - accuracy: 0.8217 - val_loss: 0.4094 - val_accuracy: 0.8731\n",
      "Epoch 18/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5806 - accuracy: 0.8226 - val_loss: 0.4066 - val_accuracy: 0.8742\n",
      "Epoch 19/100\n",
      "6000/6000 [==============================] - 75s 12ms/step - loss: 0.5788 - accuracy: 0.8232 - val_loss: 0.4042 - val_accuracy: 0.8751\n",
      "Epoch 20/100\n",
      "6000/6000 [==============================] - 75s 13ms/step - loss: 0.5768 - accuracy: 0.8238 - val_loss: 0.4022 - val_accuracy: 0.8754\n",
      "Epoch 21/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5752 - accuracy: 0.8243 - val_loss: 0.3996 - val_accuracy: 0.8761\n",
      "Epoch 22/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5729 - accuracy: 0.8250 - val_loss: 0.3977 - val_accuracy: 0.8768\n",
      "Epoch 23/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5717 - accuracy: 0.8254 - val_loss: 0.3963 - val_accuracy: 0.8772\n",
      "Epoch 24/100\n",
      "6000/6000 [==============================] - 75s 13ms/step - loss: 0.5692 - accuracy: 0.8262 - val_loss: 0.3948 - val_accuracy: 0.8776\n",
      "Epoch 25/100\n",
      "6000/6000 [==============================] - 75s 13ms/step - loss: 0.5683 - accuracy: 0.8264 - val_loss: 0.3938 - val_accuracy: 0.8781\n",
      "Epoch 26/100\n",
      "6000/6000 [==============================] - 75s 13ms/step - loss: 0.5663 - accuracy: 0.8270 - val_loss: 0.3916 - val_accuracy: 0.8784\n",
      "Epoch 27/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5646 - accuracy: 0.8275 - val_loss: 0.3888 - val_accuracy: 0.8795\n",
      "Epoch 28/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5636 - accuracy: 0.8279 - val_loss: 0.3888 - val_accuracy: 0.8793\n",
      "Epoch 29/100\n",
      "6000/6000 [==============================] - 74s 12ms/step - loss: 0.5620 - accuracy: 0.8283 - val_loss: 0.3870 - val_accuracy: 0.8802\n",
      "Epoch 30/100\n",
      "6000/6000 [==============================] - 72s 12ms/step - loss: 0.5608 - accuracy: 0.8286 - val_loss: 0.3848 - val_accuracy: 0.8806\n",
      "Epoch 31/100\n",
      "6000/6000 [==============================] - 72s 12ms/step - loss: 0.5603 - accuracy: 0.8288 - val_loss: 0.3856 - val_accuracy: 0.8802\n",
      "Epoch 32/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5586 - accuracy: 0.8293 - val_loss: 0.3839 - val_accuracy: 0.8807\n",
      "Epoch 33/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5573 - accuracy: 0.8297 - val_loss: 0.3820 - val_accuracy: 0.8814\n",
      "Epoch 34/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5568 - accuracy: 0.8299 - val_loss: 0.3805 - val_accuracy: 0.8818\n",
      "Epoch 35/100\n",
      "6000/6000 [==============================] - 75s 12ms/step - loss: 0.5556 - accuracy: 0.8301 - val_loss: 0.3801 - val_accuracy: 0.8822\n",
      "Epoch 36/100\n",
      "6000/6000 [==============================] - 75s 12ms/step - loss: 0.5548 - accuracy: 0.8305 - val_loss: 0.3795 - val_accuracy: 0.8822\n",
      "Epoch 37/100\n",
      "6000/6000 [==============================] - 75s 13ms/step - loss: 0.5541 - accuracy: 0.8307 - val_loss: 0.3771 - val_accuracy: 0.8828\n",
      "Epoch 38/100\n",
      "6000/6000 [==============================] - 75s 12ms/step - loss: 0.5527 - accuracy: 0.8310 - val_loss: 0.3767 - val_accuracy: 0.8831\n",
      "Epoch 39/100\n",
      "6000/6000 [==============================] - 74s 12ms/step - loss: 0.5516 - accuracy: 0.8315 - val_loss: 0.3763 - val_accuracy: 0.8832\n",
      "Epoch 40/100\n",
      "6000/6000 [==============================] - 75s 12ms/step - loss: 0.5508 - accuracy: 0.8316 - val_loss: 0.3759 - val_accuracy: 0.8836\n",
      "Epoch 41/100\n",
      "6000/6000 [==============================] - 75s 13ms/step - loss: 0.5500 - accuracy: 0.8319 - val_loss: 0.3747 - val_accuracy: 0.8838\n",
      "Epoch 42/100\n",
      "6000/6000 [==============================] - 75s 12ms/step - loss: 0.5492 - accuracy: 0.8321 - val_loss: 0.3747 - val_accuracy: 0.8837\n",
      "Epoch 43/100\n",
      "6000/6000 [==============================] - 74s 12ms/step - loss: 0.5494 - accuracy: 0.8322 - val_loss: 0.3719 - val_accuracy: 0.8846\n",
      "Epoch 44/100\n",
      "6000/6000 [==============================] - 74s 12ms/step - loss: 0.5474 - accuracy: 0.8327 - val_loss: 0.3712 - val_accuracy: 0.8847\n",
      "Epoch 45/100\n",
      "6000/6000 [==============================] - 75s 13ms/step - loss: 0.5471 - accuracy: 0.8328 - val_loss: 0.3713 - val_accuracy: 0.8847\n",
      "Epoch 46/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5457 - accuracy: 0.8332 - val_loss: 0.3703 - val_accuracy: 0.8851\n",
      "Epoch 47/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5456 - accuracy: 0.8334 - val_loss: 0.3681 - val_accuracy: 0.8857\n",
      "Epoch 48/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5449 - accuracy: 0.8336 - val_loss: 0.3682 - val_accuracy: 0.8855\n",
      "Epoch 49/100\n",
      "6000/6000 [==============================] - 75s 13ms/step - loss: 0.5444 - accuracy: 0.8337 - val_loss: 0.3685 - val_accuracy: 0.8860\n",
      "Epoch 50/100\n",
      "6000/6000 [==============================] - 75s 12ms/step - loss: 0.5441 - accuracy: 0.8339 - val_loss: 0.3680 - val_accuracy: 0.8857\n",
      "Epoch 51/100\n",
      "6000/6000 [==============================] - 75s 12ms/step - loss: 0.5430 - accuracy: 0.8341 - val_loss: 0.3665 - val_accuracy: 0.8860\n",
      "Epoch 52/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5423 - accuracy: 0.8343 - val_loss: 0.3664 - val_accuracy: 0.8861\n",
      "Epoch 53/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5419 - accuracy: 0.8343 - val_loss: 0.3662 - val_accuracy: 0.8864\n",
      "Epoch 54/100\n",
      "6000/6000 [==============================] - 75s 12ms/step - loss: 0.5409 - accuracy: 0.8347 - val_loss: 0.3644 - val_accuracy: 0.8865\n",
      "Epoch 55/100\n",
      "6000/6000 [==============================] - 75s 13ms/step - loss: 0.5403 - accuracy: 0.8349 - val_loss: 0.3648 - val_accuracy: 0.8866\n",
      "Epoch 56/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5396 - accuracy: 0.8352 - val_loss: 0.3641 - val_accuracy: 0.8868\n",
      "Epoch 57/100\n",
      "6000/6000 [==============================] - 75s 12ms/step - loss: 0.5394 - accuracy: 0.8353 - val_loss: 0.3631 - val_accuracy: 0.8873\n",
      "Epoch 58/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5396 - accuracy: 0.8352 - val_loss: 0.3641 - val_accuracy: 0.8868\n",
      "Epoch 59/100\n",
      "6000/6000 [==============================] - 75s 12ms/step - loss: 0.5377 - accuracy: 0.8357 - val_loss: 0.3617 - val_accuracy: 0.8876\n",
      "Epoch 60/100\n",
      "6000/6000 [==============================] - 74s 12ms/step - loss: 0.5376 - accuracy: 0.8357 - val_loss: 0.3617 - val_accuracy: 0.8876\n",
      "Epoch 61/100\n",
      "6000/6000 [==============================] - 75s 12ms/step - loss: 0.5373 - accuracy: 0.8359 - val_loss: 0.3615 - val_accuracy: 0.8875\n",
      "Epoch 62/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5368 - accuracy: 0.8360 - val_loss: 0.3614 - val_accuracy: 0.8874\n",
      "Epoch 63/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5364 - accuracy: 0.8362 - val_loss: 0.3605 - val_accuracy: 0.8877\n",
      "Epoch 64/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5355 - accuracy: 0.8365 - val_loss: 0.3612 - val_accuracy: 0.8873\n",
      "Epoch 65/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5350 - accuracy: 0.8366 - val_loss: 0.3593 - val_accuracy: 0.8884\n",
      "Epoch 66/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5343 - accuracy: 0.8367 - val_loss: 0.3595 - val_accuracy: 0.8882\n",
      "Epoch 67/100\n",
      "6000/6000 [==============================] - 75s 12ms/step - loss: 0.5344 - accuracy: 0.8368 - val_loss: 0.3584 - val_accuracy: 0.8886\n",
      "Epoch 68/100\n",
      "6000/6000 [==============================] - 74s 12ms/step - loss: 0.5344 - accuracy: 0.8367 - val_loss: 0.3582 - val_accuracy: 0.8886\n",
      "Epoch 69/100\n",
      "6000/6000 [==============================] - 74s 12ms/step - loss: 0.5338 - accuracy: 0.8370 - val_loss: 0.3587 - val_accuracy: 0.8882\n",
      "Epoch 70/100\n",
      "6000/6000 [==============================] - 74s 12ms/step - loss: 0.5331 - accuracy: 0.8371 - val_loss: 0.3582 - val_accuracy: 0.8887\n",
      "Epoch 71/100\n",
      "6000/6000 [==============================] - 74s 12ms/step - loss: 0.5332 - accuracy: 0.8372 - val_loss: 0.3566 - val_accuracy: 0.8891\n",
      "Epoch 72/100\n",
      "6000/6000 [==============================] - 75s 12ms/step - loss: 0.5328 - accuracy: 0.8373 - val_loss: 0.3569 - val_accuracy: 0.8891\n",
      "Epoch 73/100\n",
      "6000/6000 [==============================] - 75s 12ms/step - loss: 0.5316 - accuracy: 0.8375 - val_loss: 0.3572 - val_accuracy: 0.8889\n",
      "Epoch 74/100\n",
      "6000/6000 [==============================] - 74s 12ms/step - loss: 0.5311 - accuracy: 0.8378 - val_loss: 0.3562 - val_accuracy: 0.8893\n",
      "Epoch 75/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5311 - accuracy: 0.8378 - val_loss: 0.3560 - val_accuracy: 0.8891\n",
      "Epoch 76/100\n",
      "6000/6000 [==============================] - 74s 12ms/step - loss: 0.5304 - accuracy: 0.8379 - val_loss: 0.3558 - val_accuracy: 0.8893\n",
      "Epoch 77/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5298 - accuracy: 0.8381 - val_loss: 0.3559 - val_accuracy: 0.8891\n",
      "Epoch 78/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5297 - accuracy: 0.8383 - val_loss: 0.3557 - val_accuracy: 0.8893\n",
      "Epoch 79/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5293 - accuracy: 0.8384 - val_loss: 0.3535 - val_accuracy: 0.8897\n",
      "Epoch 80/100\n",
      "6000/6000 [==============================] - 75s 13ms/step - loss: 0.5295 - accuracy: 0.8383 - val_loss: 0.3537 - val_accuracy: 0.8900\n",
      "Epoch 81/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5291 - accuracy: 0.8384 - val_loss: 0.3541 - val_accuracy: 0.8897\n",
      "Epoch 82/100\n",
      "6000/6000 [==============================] - 74s 12ms/step - loss: 0.5284 - accuracy: 0.8386 - val_loss: 0.3538 - val_accuracy: 0.8900\n",
      "Epoch 83/100\n",
      "6000/6000 [==============================] - 75s 13ms/step - loss: 0.5277 - accuracy: 0.8388 - val_loss: 0.3524 - val_accuracy: 0.8903\n",
      "Epoch 84/100\n",
      "6000/6000 [==============================] - 75s 13ms/step - loss: 0.5276 - accuracy: 0.8389 - val_loss: 0.3542 - val_accuracy: 0.8897\n",
      "Epoch 85/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5278 - accuracy: 0.8388 - val_loss: 0.3510 - val_accuracy: 0.8905\n",
      "Epoch 86/100\n",
      "6000/6000 [==============================] - 75s 13ms/step - loss: 0.5267 - accuracy: 0.8390 - val_loss: 0.3515 - val_accuracy: 0.8908\n",
      "Epoch 87/100\n",
      "6000/6000 [==============================] - 75s 13ms/step - loss: 0.5270 - accuracy: 0.8390 - val_loss: 0.3517 - val_accuracy: 0.8907\n",
      "Epoch 88/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5266 - accuracy: 0.8392 - val_loss: 0.3510 - val_accuracy: 0.8908\n",
      "Epoch 89/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5258 - accuracy: 0.8393 - val_loss: 0.3508 - val_accuracy: 0.8909\n",
      "Epoch 90/100\n",
      "6000/6000 [==============================] - 75s 13ms/step - loss: 0.5261 - accuracy: 0.8393 - val_loss: 0.3508 - val_accuracy: 0.8909\n",
      "Epoch 91/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5257 - accuracy: 0.8396 - val_loss: 0.3508 - val_accuracy: 0.8905\n",
      "Epoch 92/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5255 - accuracy: 0.8395 - val_loss: 0.3498 - val_accuracy: 0.8912\n",
      "Epoch 93/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5252 - accuracy: 0.8394 - val_loss: 0.3498 - val_accuracy: 0.8911\n",
      "Epoch 94/100\n",
      "6000/6000 [==============================] - 74s 12ms/step - loss: 0.5248 - accuracy: 0.8397 - val_loss: 0.3506 - val_accuracy: 0.8908\n",
      "Epoch 95/100\n",
      "6000/6000 [==============================] - 73s 12ms/step - loss: 0.5240 - accuracy: 0.8398 - val_loss: 0.3503 - val_accuracy: 0.8909\n",
      "Epoch 96/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5244 - accuracy: 0.8398 - val_loss: 0.3488 - val_accuracy: 0.8914\n",
      "Epoch 97/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5242 - accuracy: 0.8398 - val_loss: 0.3499 - val_accuracy: 0.8910\n",
      "Epoch 98/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5237 - accuracy: 0.8400 - val_loss: 0.3489 - val_accuracy: 0.8915\n",
      "Epoch 99/100\n",
      "6000/6000 [==============================] - 76s 13ms/step - loss: 0.5238 - accuracy: 0.8400 - val_loss: 0.3486 - val_accuracy: 0.8913\n",
      "Epoch 100/100\n",
      "6000/6000 [==============================] - 77s 13ms/step - loss: 0.5238 - accuracy: 0.8399 - val_loss: 0.3479 - val_accuracy: 0.8917\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adamax', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.fit([encoder_input_data_train, decoder_input_data_train],  # training data\n",
    "          decoder_target_data_train,                       # labels (left shift of the target sequences)\n",
    "              batch_size=16, epochs=100, validation_data=([encoder_input_data_valid, decoder_input_data_valid], decoder_target_data_valid),  callbacks=[callback])\n",
    "\n",
    "model.save('seq2seq_tune.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "-zurtPEyBtY9"
   },
   "outputs": [],
   "source": [
    "input_test_texts = input_texts[test_indices]\n",
    "target_texts = numpy.array(target_texts)\n",
    "target_test_texts = target_texts[test_indices]\n",
    "predicted_sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "l7GCoqYaBtY9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "ArVSN3kjBtY-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "YEK9H-SQBtY-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "QzpyTx05BtY-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "English:        he paid with a credit card\n",
      "Spanish (true):  el pago con una tarjeta de credito\n",
      "Spanish (pred):  er se cadp\n",
      "-\n",
      "English:        what cant you do\n",
      "Spanish (true):  que no puedes hacer\n",
      "Spanish (pred):  vie \ta pieues goce\n",
      "-\n",
      "English:        just tell me what youd like me to do\n",
      "Spanish (true):  solo dime que quieres que haga\n",
      "Spanish (pred):  uajo ue erro te unje vie te yisto\n",
      "-\n",
      "English:        is there a souvenir shop in the hotel\n",
      "Spanish (true):  hay una tienda de recuerdos en el hotel\n",
      "Spanish (pred):  goq i\t paca ue ro cobezo e\t ro coiso\n",
      "-\n",
      "English:        weeds sprang up in the garden\n",
      "Spanish (true):  las malas hierbas aparecieron subitamente en el jardin\n",
      "Spanish (pred):  ros o\n",
      "-\n",
      "English:        its easy to get confused\n",
      "Spanish (true):  es facil equivocarse\n",
      "Spanish (pred):  es uedosnoua co\tsoua po\n",
      "-\n",
      "English:        keep an eye on my bag for a while\n",
      "Spanish (true):  vigila mi bolso un momento\n",
      "Spanish (pred):  do\n",
      "-\n",
      "English:        toms room wasnt clean\n",
      "Spanish (true):  el cuarto de tom no estaba limpio\n",
      "Spanish (pred):  ro doro ue tad \ta estobo e\t er core\n",
      "-\n",
      "English:        this clock isnt working\n",
      "Spanish (true):  este reloj no funciona\n",
      "Spanish (pred):  este cnb\n",
      "-\n",
      "English:        i think its time for me to consult a lawyer\n",
      "Spanish (true):  creo que es hora de que consulte a un abogado\n",
      "Spanish (pred):  c\n",
      "-\n",
      "English:        have you seen a brown wallet around here\n",
      "Spanish (true):  has visto por aqui una cartera marron\n",
      "Spanish (pred):  gos hnsta i\to coso ue ore\t ue ro cobezo\n",
      "-\n",
      "English:        sweat is dripping from his face\n",
      "Spanish (true):  el sudor gotea de su cara\n",
      "Spanish (pred):  ro do\to es er dosa\n",
      "-\n",
      "English:        you are lying to me\n",
      "Spanish (true):  me estas mintiendo\n",
      "Spanish (pred):  estos dosto\tua oryi\tas ue erro\n",
      "-\n",
      "English:        look at that mountain\n",
      "Spanish (true):  miren esa montana\n",
      "Spanish (pred):  dn\n",
      "-\n",
      "English:        he goes to school on foot\n",
      "Spanish (true):  el va a la escuela caminando\n",
      "Spanish (pred):  er hnhe e\t ro cobezo ue ro cobezo\n",
      "-\n",
      "English:        its strange\n",
      "Spanish (true):  es extrano\n",
      "Spanish (pred):  es i\to pe\n",
      "-\n",
      "English:        i wasnt feeling well\n",
      "Spanish (true):  no me sentia bien\n",
      "Spanish (pred):  \ta estobo espe\n",
      "-\n",
      "English:        money does not smell\n",
      "Spanish (true):  el dinero no huele\n",
      "Spanish (pred):  er \tn\ta \ta se co\ta\n",
      "-\n",
      "English:        tom wrote mary a love letter\n",
      "Spanish (true):  tom le escribio una carta de amor a maria\n",
      "Spanish (pred):  tad re cadp\n",
      "-\n",
      "English:        did he really do that\n",
      "Spanish (true):  el realmente hizo eso\n",
      "Spanish (pred):  uejo vie \ta ra goyos\n",
      "-\n",
      "English:        they talked during the movie\n",
      "Spanish (true):  ellos hablaban durante la pelicula\n",
      "Spanish (pred):  erras re oca\tto\n",
      "-\n",
      "English:        tom is still not married\n",
      "Spanish (true):  tom aun no esta casado\n",
      "Spanish (pred):  tad \ta esto doroua ue do\n",
      "-\n",
      "English:        heres my album\n",
      "Spanish (true):  aqui esta mi album\n",
      "Spanish (pred):  ovin esto dn coso\n",
      "-\n",
      "English:        i took a picture of her\n",
      "Spanish (true):  tome una fotografia de ella\n",
      "Spanish (pred):  qa hn i\t po\n",
      "-\n",
      "English:        i dont feel like walking so fast\n",
      "Spanish (true):  no tengo ganas de caminar tan rapido\n",
      "Spanish (pred):  \ta de yisto codn\to\n",
      "-\n",
      "English:        tom is always happy\n",
      "Spanish (true):  tom siempre esta feliz\n",
      "Spanish (pred):  tad snedp\n",
      "-\n",
      "English:        we wont know until october\n",
      "Spanish (true):  no lo sabremos hasta octubre\n",
      "Spanish (pred):  \ta sabnodas pa\n",
      "-\n",
      "English:        the door was locked from the inside\n",
      "Spanish (true):  la puerta estaba cerrada desde adentro\n",
      "Spanish (pred):  er pe\n",
      "-\n",
      "English:        tom didnt get to school in time\n",
      "Spanish (true):  tom no llego a tiempo a la escuela\n",
      "Spanish (pred):  tad \ta cadna o tad e\t er co\te\n",
      "-\n",
      "English:        why do you study so hard\n",
      "Spanish (true):  por que estudias tanto\n",
      "Spanish (pred):  pa\n",
      "-\n",
      "English:        she had to use her dictionary many times\n",
      "Spanish (true):  ella tuvo que usar el diccionario muchas veces\n",
      "Spanish (pred):  erro tne\te vie o\n",
      "-\n",
      "English:        leave the hard tasks to me\n",
      "Spanish (true):  dejame las tareas complicadas a mi\n",
      "Spanish (pred):  uejo ro coso ue ras unos\n",
      "-\n",
      "English:        it took me several hours to write it\n",
      "Spanish (true):  me llevo varias horas para escribirlo\n",
      "Spanish (pred):  es unfncnr ue cadp\n",
      "-\n",
      "English:        do you have a credit card\n",
      "Spanish (true):  tienes una tarjeta de credito\n",
      "Spanish (pred):  tne\tes i\t paca ue o\tas\n",
      "-\n",
      "English:        the house was trashed\n",
      "Spanish (true):  la casa quedo hecha una pena\n",
      "Spanish (pred):  er cie\n",
      "-\n",
      "English:        you mustve seen them there\n",
      "Spanish (true):  tienes que haberlos visto alli\n",
      "Spanish (pred):  uebes uo\n",
      "-\n",
      "English:        tom cooked breakfast\n",
      "Spanish (true):  tomas preparo el desayuno\n",
      "Spanish (pred):  tad se cadp\n",
      "-\n",
      "English:        it was excessive\n",
      "Spanish (true):  fue excesivo\n",
      "Spanish (pred):  fie i\to co\toune\tse\n",
      "-\n",
      "English:        please come next friday\n",
      "Spanish (true):  por favor ven el proximo viernes\n",
      "Spanish (pred):  pa\n",
      "-\n",
      "English:        do you believe in eternal life\n",
      "Spanish (true):  creen en la vida eterna\n",
      "Spanish (pred):  c\n",
      "-\n",
      "English:        tom tripled his investment in six months\n",
      "Spanish (true):  tom triplico su inversion en seis meses\n",
      "Spanish (pred):  tad se cadp\n",
      "-\n",
      "English:        i thought tom and mary were twins\n",
      "Spanish (true):  pense que tom y maria eran gemelos\n",
      "Spanish (pred):  pe\tse vie tad estobo diq uesocne\tua\n",
      "-\n",
      "English:        we havent been introduced yet\n",
      "Spanish (true):  aun no nos han presentado\n",
      "Spanish (pred):  \ta gobedas dicga un\te\n",
      "-\n",
      "English:        they are deliberating what to do next\n",
      "Spanish (true):  estan pensando que hacer a continuacion\n",
      "Spanish (pred):  erras esto\t estiuno\tua ovin po\n",
      "-\n",
      "English:        please keep it a secret\n",
      "Spanish (true):  mantenelo en secreto por favor\n",
      "Spanish (pred):  pa\n",
      "-\n",
      "English:        the fire is out\n",
      "Spanish (true):  el fuego se extinguio\n",
      "Spanish (pred):  ro cniuou esto o\tas\n",
      "-\n",
      "English:        i can still hear you\n",
      "Spanish (true):  todavia puedo oirte\n",
      "Spanish (pred):  pieua he\n",
      "-\n",
      "English:        its one of those moments\n",
      "Spanish (true):  es uno de esos momentos\n",
      "Spanish (pred):  es diq ca\tacnuoua ue ras unos\n",
      "-\n",
      "English:        your words are very important\n",
      "Spanish (true):  tus palabras son muy importantes\n",
      "Spanish (pred):  tis pou\n",
      "-\n",
      "English:        now everythings going smoothly\n",
      "Spanish (true):  ahora todo va bien\n",
      "Spanish (pred):  oga\n",
      "-\n",
      "English:        tom followed marys instructions\n",
      "Spanish (true):  tom siguio las instrucciones de mary\n",
      "Spanish (pred):  tad se cadp\n",
      "-\n",
      "English:        are you still sleepy\n",
      "Spanish (true):  todavia tenes sueno\n",
      "Spanish (pred):  tauohno estos co\tsoua\n",
      "-\n",
      "English:        he became a police officer\n",
      "Spanish (true):  se hizo policia\n",
      "Spanish (pred):  er se cadp\n",
      "-\n",
      "English:        this is where my family used to live\n",
      "Spanish (true):  aqui es donde mi familia solia vivir\n",
      "Spanish (pred):  este es dos uedosnoua to\n",
      "-\n",
      "English:        the sun was about to set\n",
      "Spanish (true):  se iba a poner el sol\n",
      "Spanish (pred):  er cida estobo p\n",
      "-\n",
      "English:        the results were clear\n",
      "Spanish (true):  los resultados estaban claros\n",
      "Spanish (pred):  ros ca\n",
      "-\n",
      "English:        does truth matter\n",
      "Spanish (true):  la verdad importa\n",
      "Spanish (pred):  \ta sa\t dos unsta\n",
      "-\n",
      "English:        we must learn to work together\n",
      "Spanish (true):  debemos aprender a trabajar juntos\n",
      "Spanish (pred):  uebedas gobro\n",
      "-\n",
      "English:        i have a fish tank\n",
      "Spanish (true):  tengo un acuario\n",
      "Spanish (pred):  te\tya i\t paca ue ras unos\n",
      "-\n",
      "English:        weve been very busy for the last few days\n",
      "Spanish (true):  hemos estado bastante ocupados estos ultimos dias\n",
      "Spanish (pred):  gedas gecga uesosnoua oiq o\tas ue sa\t gnjas\n",
      "-\n",
      "English:        it costs three dollars\n",
      "Spanish (true):  cuesta tres mangos\n",
      "Spanish (pred):  ca\tsnyine e\t ros unez ue ros \tn\tas\n",
      "-\n",
      "English:        why dont you try it\n",
      "Spanish (true):  por que no lo intentas\n",
      "Spanish (pred):  pa\n",
      "-\n",
      "English:        i just dont want you to have it\n",
      "Spanish (true):  sencillamente no quiero que lo tengas\n",
      "Spanish (pred):  sara vine\n",
      "-\n",
      "English:        i like chicken\n",
      "Spanish (true):  me gusta el pollo\n",
      "Spanish (pred):  de yisto ro cne\tuo\n",
      "-\n",
      "English:        what are those\n",
      "Spanish (true):  que son esos\n",
      "Spanish (pred):  vie esto\t rne\tas\n",
      "-\n",
      "English:        tom got angry with the high school students\n",
      "Spanish (true):  tom se enfado con los estudiantes de instituto\n",
      "Spanish (pred):  tad se cadp\n",
      "-\n",
      "English:        could you speak more slowly please\n",
      "Spanish (true):  puede hablar mas despacio por favor\n",
      "Spanish (pred):  pau\n",
      "-\n",
      "English:        he misses his family very much\n",
      "Spanish (true):  extrana mucho a su familia\n",
      "Spanish (pred):  er de una i\to coso ue do\n",
      "-\n",
      "English:        tom is memorizing a poem\n",
      "Spanish (true):  tom se esta memorizando un poema\n",
      "Spanish (pred):  tad esto e\t er pacn\te\n",
      "-\n",
      "English:        i didnt plan on doing that\n",
      "Spanish (true):  no lo habia planeado\n",
      "Spanish (pred):  \ta pasn o tad vie \ta gobno esta\n",
      "-\n",
      "English:        thats none of your business\n",
      "Spanish (true):  esto no es asunto tuyo\n",
      "Spanish (pred):  esa \ta es ti carpo\n",
      "-\n",
      "English:        how long have you known him\n",
      "Spanish (true):  cuanto hace que lo conoces\n",
      "Spanish (pred):  cio\tta tnedpa te unste orrn\n",
      "-\n",
      "English:        he thinks of nothing but making money\n",
      "Spanish (true):  el no piensa en nada mas que en ganar dinero\n",
      "Spanish (pred):  er po\n",
      "-\n",
      "English:        i carried the heavy bag on my back\n",
      "Spanish (true):  lleve a la espalda la bolsa pesada\n",
      "Spanish (pred):  re paun er co\te ue ro cobezo ue do\n",
      "-\n",
      "English:        she is dressed like a bride\n",
      "Spanish (true):  ella esta vestida de novia\n",
      "Spanish (pred):  erro esto o\tastidb\n",
      "-\n",
      "English:        opinions vary from person to person\n",
      "Spanish (true):  las opiniones varian de una persona a otra\n",
      "Spanish (pred):  ro \tacncno esto doso\tua ue do\n",
      "-\n",
      "English:        nobody wants it\n",
      "Spanish (true):  nadie lo quiere\n",
      "Spanish (pred):  \toune vine\n",
      "-\n",
      "English:        tom sang\n",
      "Spanish (true):  tom canto\n",
      "Spanish (pred):  tad se co\n",
      "-\n",
      "English:        is it very far from here\n",
      "Spanish (true):  es muy lejos de aqui\n",
      "Spanish (pred):  es diq batnrrosa ue dn\n",
      "-\n",
      "English:        the school is on the hill\n",
      "Spanish (true):  la escuela se encuentra en la colina\n",
      "Spanish (pred):  ro cobo esto e\t ro doso\n",
      "-\n",
      "English:        i will not see him any more\n",
      "Spanish (true):  ya no lo vere mas\n",
      "Spanish (pred):  \ta ra ho\n",
      "-\n",
      "English:        tom sat patiently\n",
      "Spanish (true):  tomas se sento pacientemente\n",
      "Spanish (pred):  tad se cosa ca\t er pe\n",
      "-\n",
      "English:        why dont we finish this later\n",
      "Spanish (true):  por que no acabamos esto mas tarde\n",
      "Spanish (pred):  pa\n",
      "-\n",
      "English:        tom told the judge that he was innocent\n",
      "Spanish (true):  tom le dijo al juez que el era inocente\n",
      "Spanish (pred):  tad re unja vie estobo e\t er cadpate\tte ue ro co\to\n",
      "-\n",
      "English:        come to my house at the end of this month\n",
      "Spanish (true):  ven a mi casa al termino de este mes\n",
      "Spanish (pred):  he\tyo este do\n",
      "-\n",
      "English:        do you like your coworkers\n",
      "Spanish (true):  te gustan tus colegas\n",
      "Spanish (pred):  te yisto ti ca\n",
      "-\n",
      "English:        can you tell me what this is\n",
      "Spanish (true):  puedes decirme que es esto\n",
      "Spanish (pred):  pau\n",
      "-\n",
      "English:        its my last offer\n",
      "Spanish (true):  es mi ultima oferta\n",
      "Spanish (pred):  es dn obnyo e\t ro coso\n",
      "-\n",
      "English:        were not your enemies\n",
      "Spanish (true):  no somos tus enemigos\n",
      "Spanish (pred):  \ta estodas sa\t ca\tsouas\n",
      "-\n",
      "English:        i thought the game was over\n",
      "Spanish (true):  pensaba que el juego habia terminado\n",
      "Spanish (pred):  pe\tse vie er sobna estobo e\t er dos\n",
      "-\n",
      "English:        can i have some of these\n",
      "Spanish (true):  puede darme algunos de estos\n",
      "Spanish (pred):  pieua he\n",
      "-\n",
      "English:        tom doesnt do anything i ask him to do\n",
      "Spanish (true):  tom no hace nada de lo que le pido\n",
      "Spanish (pred):  tad \ta co\tece \touo vie goce\n",
      "-\n",
      "English:        he is the least likely to come\n",
      "Spanish (true):  el es el menos probable que venga\n",
      "Spanish (pred):  er es ro prtnto po\n",
      "-\n",
      "English:        be good\n",
      "Spanish (true):  sean buenos\n",
      "Spanish (pred):  se\t fornz\n",
      "-\n",
      "English:        i always take a shower in the morning\n",
      "Spanish (true):  siempre me pego una ducha por la manana\n",
      "Spanish (pred):  snedp\n",
      "-\n",
      "English:        she makes me happy\n",
      "Spanish (true):  ella me hace feliz\n",
      "Spanish (pred):  erro hnhe o sis pou\n",
      "-\n",
      "English:        tom hasnt been there yet\n",
      "Spanish (true):  tom no estuvo aun ahi\n",
      "Spanish (pred):  tad \ta go gocnua ogn\n",
      "-\n",
      "English:        what would you say then\n",
      "Spanish (true):  que dirias en ese caso\n",
      "Spanish (pred):  vie gobno\n",
      "-\n",
      "English:        i want to be safe\n",
      "Spanish (true):  quiero estar segura\n",
      "Spanish (pred):  vine\n",
      "-\n",
      "English:        tom promised me hed do that\n",
      "Spanish (true):  tom me prometio que lo haria\n",
      "Spanish (pred):  tad re p\n",
      "-\n",
      "English:        she sat on the bank\n",
      "Spanish (true):  se sento en el banco\n",
      "Spanish (pred):  erro re una ro coso\n",
      "-\n",
      "English:        why do you love me\n",
      "Spanish (true):  por que me amas\n",
      "Spanish (pred):  pa\n",
      "-\n",
      "English:        what if something gets broken\n",
      "Spanish (true):  y si algo se rompe\n",
      "Spanish (pred):  vie tne\te to\t po\n",
      "-\n",
      "English:        ive done ok\n",
      "Spanish (true):  lo hice bien\n",
      "Spanish (pred):  ge estoua e\t coso\n",
      "-\n",
      "English:        why is tom wet\n",
      "Spanish (true):  por que esta tom mojado\n",
      "Spanish (pred):  pa\n",
      "-\n",
      "English:        he made up his mind to be a teacher\n",
      "Spanish (true):  el decidio ser profesor\n",
      "Spanish (pred):  er ueja vie e\n",
      "-\n",
      "English:        tom woke up in the middle of the night\n",
      "Spanish (true):  tom se desperto a la mitad de la noche\n",
      "Spanish (pred):  tad re oca\tsaja er di\tea e\t er ci\te\n",
      "-\n",
      "English:        do you drink\n",
      "Spanish (true):  bebeis\n",
      "Spanish (pred):  te yostos\n",
      "-\n",
      "English:        tom pushed mary down the stairs\n",
      "Spanish (true):  tom empujo a mary por las escaleras\n",
      "Spanish (pred):  tad re cadp\n",
      "-\n",
      "English:        i want you to be my friend again\n",
      "Spanish (true):  quiero que seas mi amigo de nuevo\n",
      "Spanish (pred):  vine\n",
      "-\n",
      "English:        how often do you talk with your parents\n",
      "Spanish (true):  con que frecuencia hablas con tus padres\n",
      "Spanish (pred):  ca\t vie t\n",
      "-\n",
      "English:        youre free to leave\n",
      "Spanish (true):  eres libre de marcharte\n",
      "Spanish (pred):  estos uesosnoua fernz\n",
      "-\n",
      "English:        he often appears on tv\n",
      "Spanish (true):  el a menudo aparece en television\n",
      "Spanish (pred):  er se ca\tacna or paca ue ro coiso\n",
      "-\n",
      "English:        tom couldnt understand anything mary said\n",
      "Spanish (true):  tom no pudo entender nada de lo que dijo mary\n",
      "Spanish (pred):  tad \ta pauno gobe\n",
      "-\n",
      "English:        im not sure how to pronounce this word\n",
      "Spanish (true):  no estoy seguro de como pronunciar esta palabra\n",
      "Spanish (pred):  \ta estaq seyi\n",
      "-\n",
      "English:        what made you so angry\n",
      "Spanish (true):  que te puso tan furioso\n",
      "Spanish (pred):  vie gos gocne\tua i\to pe\n",
      "-\n",
      "English:        this is a trap\n",
      "Spanish (true):  esto es una trampa\n",
      "Spanish (pred):  esta es i\to pe\n",
      "-\n",
      "English:        what would the world be like without women\n",
      "Spanish (true):  como seria el mundo sin mujeres\n",
      "Spanish (pred):  vie po\n",
      "-\n",
      "English:        tom didnt have a pencil\n",
      "Spanish (true):  tom no tenia lapiz\n",
      "Spanish (pred):  tad \ta tne\te i\t peca\n",
      "-\n",
      "English:        she took part in the contest\n",
      "Spanish (true):  ella participo en el concurso\n",
      "Spanish (pred):  erro re e\tse\ta o ro esciero\n",
      "-\n",
      "English:        tom asked me how tall i was\n",
      "Spanish (true):  tom me pregunto que tan alto soy\n",
      "Spanish (pred):  tad re pnuna o do\n",
      "-\n",
      "English:        its very late\n",
      "Spanish (true):  es muy tarde\n",
      "Spanish (pred):  es diq o\n",
      "-\n",
      "English:        hi how are you doing\n",
      "Spanish (true):  hola que tal\n",
      "Spanish (pred):  o he\n",
      "-\n",
      "English:        you dont like love stories do you\n",
      "Spanish (true):  a ti no te gustan las historias de amor no es cierto\n",
      "Spanish (pred):  \ta te yisto cada ca\ttnya\n",
      "-\n",
      "English:        some of them are my friends\n",
      "Spanish (true):  algunos de ellos son mis amigos\n",
      "Spanish (pred):  oryi\tas ue estos do\to\tos sa\t dns p\n",
      "-\n",
      "English:        you need to stop doing that\n",
      "Spanish (true):  tienes que dejar de hacer eso\n",
      "Spanish (pred):  \tecesntos gobe\n",
      "-\n",
      "English:        my friends dont play tennis\n",
      "Spanish (true):  mis amigos no juegan tenis\n",
      "Spanish (pred):  dns pou\n",
      "-\n",
      "English:        we must save electricity\n",
      "Spanish (true):  debemos ahorrar electricidad\n",
      "Spanish (pred):  uebedas uecn\n",
      "-\n",
      "English:        im not able to fix the computer\n",
      "Spanish (true):  no puedo reparar el ordenador\n",
      "Spanish (pred):  \ta estaq seyi\n",
      "-\n",
      "English:        this book is about education\n",
      "Spanish (true):  este libro es sobre educacion\n",
      "Spanish (pred):  este rnb\n",
      "-\n",
      "English:        what a collection\n",
      "Spanish (true):  que coleccion\n",
      "Spanish (pred):  vie copocne\tte\n",
      "-\n",
      "English:        were not barbarians\n",
      "Spanish (true):  no somos barbaros\n",
      "Spanish (pred):  \ta estodas o\n",
      "-\n",
      "English:        do you play tennis\n",
      "Spanish (true):  juegas tenis\n",
      "Spanish (pred):  te oreyoste er core\n",
      "-\n",
      "English:        who called them\n",
      "Spanish (true):  quien los llamo\n",
      "Spanish (pred):  vine\t re go rasta\n",
      "-\n",
      "English:        could you come back a little later\n",
      "Spanish (true):  podrias volver un poco mas tarde\n",
      "Spanish (pred):  pau\n",
      "-\n",
      "English:        i just wanted to see if you knew\n",
      "Spanish (true):  solo queria ver si lo sabias\n",
      "Spanish (pred):  sara vinse uecn\n",
      "-\n",
      "English:        this castle was built in\n",
      "Spanish (true):  este castillo fue construido en\n",
      "Spanish (pred):  este cobeto estobo e\tfe\n",
      "-\n",
      "English:        everyone looked\n",
      "Spanish (true):  todos miraron\n",
      "Spanish (pred):  tauas sa\t uesas\n",
      "-\n",
      "English:        i wonder if anybody knows where tom is\n",
      "Spanish (true):  me pregunto si alguien sabe donde esta tom\n",
      "Spanish (pred):  de p\n",
      "-\n",
      "English:        tom treats his employees generously\n",
      "Spanish (true):  tom trata a sus empleados con generosidad\n",
      "Spanish (pred):  tad se cadp\n",
      "-\n",
      "English:        nobody ever helps me\n",
      "Spanish (true):  nunca me ayuda nadie\n",
      "Spanish (pred):  \toune de ociuo\n",
      "-\n",
      "English:        my wife is cooking right now\n",
      "Spanish (true):  ahora mi mujer esta haciendo la comida\n",
      "Spanish (pred):  dn gnja esto ore\to\tua or cn\te\n",
      "-\n",
      "English:        the view from this room is wonderful\n",
      "Spanish (true):  la vista desde esta habitacion es maravillosa\n",
      "Spanish (pred):  ro hnsnco esto co\toua e\t esto co\n",
      "-\n",
      "English:        whos your favorite guitarist\n",
      "Spanish (true):  cual es tu guitarrista preferido\n",
      "Spanish (pred):  vine\t es ti p\n",
      "-\n",
      "English:        he seems not to have realized its importance\n",
      "Spanish (true):  el parece no haber comprendido su importancia\n",
      "Spanish (pred):  er po\n",
      "-\n",
      "English:        tom paid\n",
      "Spanish (true):  tom pago\n",
      "Spanish (pred):  tad se ca\tta\n",
      "-\n",
      "English:        what book are you looking for\n",
      "Spanish (true):  que libro estas buscando\n",
      "Spanish (pred):  vie tnpa ue he\n",
      "-\n",
      "English:        are you taking vitamins\n",
      "Spanish (true):  tomas vitaminas\n",
      "Spanish (pred):  estos espe\n",
      "-\n",
      "English:        tom hasnt been living here long\n",
      "Spanish (true):  tom no ha vivido aqui por mucho tiempo\n",
      "Spanish (pred):  tad \ta go estoua e\t er ca\te\n",
      "-\n",
      "English:        how many children do you have\n",
      "Spanish (true):  cuantos hijos tienes\n",
      "Spanish (pred):  cio\ttos co\n",
      "-\n",
      "English:        i would never betray you\n",
      "Spanish (true):  yo jamas te traicionaria\n",
      "Spanish (pred):  \ti\tco de oqiuo\n",
      "-\n",
      "English:        we havent been invited yet\n",
      "Spanish (true):  no nos han invitado todavia\n",
      "Spanish (pred):  \ta gobndas \touo ue ra vie esto\n",
      "-\n",
      "English:        when will it begin\n",
      "Spanish (true):  cuando empieza\n",
      "Spanish (pred):  cio\tua harhe\n",
      "-\n",
      "English:        i called you\n",
      "Spanish (true):  te he llamado\n",
      "Spanish (pred):  ra ge cadp\n",
      "-\n",
      "English:        i live a busy life\n",
      "Spanish (true):  vivo una vida ocupada\n",
      "Spanish (pred):  de e\tco\tto i\to co\n",
      "-\n",
      "English:        all seats are reserved\n",
      "Spanish (true):  todos los asientos estan reservados\n",
      "Spanish (pred):  ras un\te\n",
      "-\n",
      "English:        whats todays menu\n",
      "Spanish (true):  que menu es hoy\n",
      "Spanish (pred):  vie esto e\t dn coso\n",
      "-\n",
      "English:        i was the happiest man on earth\n",
      "Spanish (true):  yo era el hombre mas feliz de la tierra\n",
      "Spanish (pred):  estobo e\t er co\te ue ro co\n",
      "-\n",
      "English:        i already have an ipad\n",
      "Spanish (true):  ya tengo un ipad\n",
      "Spanish (pred):  qo gnce i\t paca ue tne\t\n",
      "-\n",
      "English:        he has many enemies in the political world\n",
      "Spanish (true):  el tiene muchos enemigos en el mundo de la politica\n",
      "Spanish (pred):  er tne\te i\t odnya ue ro cobezo e\t ro cobeto\n",
      "-\n",
      "English:        no one lets me have fun anymore\n",
      "Spanish (true):  ya nadie me deja divertirme\n",
      "Spanish (pred):  \toune de uecesnto orya ue cade\n",
      "-\n",
      "English:        i dont feel like a man anymore\n",
      "Spanish (true):  ya no me siento mas como un hombre\n",
      "Spanish (pred):  \ta de yisto do\n",
      "-\n",
      "English:        i always thought tom was a canadian\n",
      "Spanish (true):  siempre pense que tom era canadiense\n",
      "Spanish (pred):  snedp\n",
      "-\n",
      "English:        the world did not recognize him\n",
      "Spanish (true):  el mundo no lo reconocio\n",
      "Spanish (pred):  er \tn\ta \ta se pe\n",
      "-\n",
      "English:        he has access to the american embassy\n",
      "Spanish (true):  el tiene acceso a la embajada de los estados unidos\n",
      "Spanish (pred):  er tne\te i\t paca ue coso pa\n",
      "-\n",
      "English:        i think youre perfect\n",
      "Spanish (true):  yo creo que eres perfecto\n",
      "Spanish (pred):  c\n",
      "-\n",
      "English:        can you frame this picture\n",
      "Spanish (true):  podria enmarcar esta foto\n",
      "Spanish (pred):  pieues ca\tte\te\n",
      "-\n",
      "English:        let me know when youre ready\n",
      "Spanish (true):  avisa cuando estes listo\n",
      "Spanish (pred):  uejo vie te vieuos estoua e\tfe\n",
      "-\n",
      "English:        may i speak to you in private\n",
      "Spanish (true):  puedo hablar contigo a solas\n",
      "Spanish (pred):  pieua p\n",
      "-\n",
      "English:        he lay face up\n",
      "Spanish (true):  el estaba tirado de espaldas\n",
      "Spanish (pred):  er po\n",
      "-\n",
      "English:        i should have come earlier\n",
      "Spanish (true):  deberia haber venido antes\n",
      "Spanish (pred):  uebe\n",
      "-\n",
      "English:        i think its time for me to wash this shirt\n",
      "Spanish (true):  creo que es hora de que lave esta camisa\n",
      "Spanish (pred):  c\n",
      "-\n",
      "English:        eating between meals is a bad habit\n",
      "Spanish (true):  picotear entre comidas es un mal habito\n",
      "Spanish (pred):  er careyna es i\to po\n",
      "-\n",
      "English:        it doesnt seem possible does it\n",
      "Spanish (true):  no parece posible verdad\n",
      "Spanish (pred):  \ta de po\n",
      "-\n",
      "English:        she caught him cheating on a test\n",
      "Spanish (true):  lo pillo haciendo trampa en una prueba\n",
      "Spanish (pred):  erro re unja i\to co\n",
      "-\n",
      "English:        she seemed to have been ill\n",
      "Spanish (true):  ella parecia haber estado enferma\n",
      "Spanish (pred):  erro de unja vie estobo este\n",
      "-\n",
      "English:        i found out how to solve the problem\n",
      "Spanish (true):  descubri como resolver el problema\n",
      "Spanish (pred):  e\tc\n",
      "-\n",
      "English:        you dont seem too happy to see me\n",
      "Spanish (true):  no pareces muy feliz de verme\n",
      "Spanish (pred):  \ta sobnos vie se ra unyos ue oqiuo\n",
      "-\n",
      "English:        i want to spend the rest of my life with you\n",
      "Spanish (true):  quiero pasar el resto de mi vida contigo\n",
      "Spanish (pred):  vine\n",
      "-\n",
      "English:        my watch gains one minute a day\n",
      "Spanish (true):  mi reloj se adelanta un minuto al dia\n",
      "Spanish (pred):  dn cijo esto e\t er bada ue po\n",
      "-\n",
      "English:        she asked him for help\n",
      "Spanish (true):  ella le pidio ayuda a el\n",
      "Spanish (pred):  erro re pnuna o iuo po\n",
      "-\n",
      "English:        why are you still in boston\n",
      "Spanish (true):  por que estas todavia en boston\n",
      "Spanish (pred):  pa\n",
      "-\n",
      "English:        im having dinner with tom tomorrow\n",
      "Spanish (true):  manana ceno con tom\n",
      "Spanish (pred):  de estaq ueso\tua do\to\to o do\to\to\n",
      "-\n",
      "English:        wheres everyone\n",
      "Spanish (true):  en donde estan todos\n",
      "Spanish (pred):  ua\tue esto er di\tua\n",
      "-\n",
      "English:        were quieter than tom\n",
      "Spanish (true):  estamos mas tranquilos que tom\n",
      "Spanish (pred):  estodas ovin po\n",
      "-\n",
      "English:        have you dug up the potatoes\n",
      "Spanish (true):  desenterraron las papas\n",
      "Spanish (pred):  gos hesta or corerra ue ro cne\n",
      "-\n",
      "English:        you dont have to work on sundays\n",
      "Spanish (true):  no tienes que trabajar los domingos\n",
      "Spanish (pred):  \ta tne\tes vie n\n",
      "-\n",
      "English:        you ought to know better at your age\n",
      "Spanish (true):  deberias de saberlo mejor a tu edad\n",
      "Spanish (pred):  uebe\n",
      "-\n",
      "English:        i dont like stubborn people\n",
      "Spanish (true):  no me gusta las personas testarudas\n",
      "Spanish (pred):  \ta de yisto co\n",
      "-\n",
      "English:        i added water\n",
      "Spanish (true):  anadi agua\n",
      "Spanish (pred):  ra ge e\tca\tt\n",
      "-\n",
      "English:        youre either with me or youre against me\n",
      "Spanish (true):  estas conmigo o contra mi\n",
      "Spanish (pred):  e\n",
      "-\n",
      "English:        tom has no choice but to go\n",
      "Spanish (true):  tom no tiene mas opcion que ir\n",
      "Spanish (pred):  tad \ta tne\te \touo vie po\n",
      "-\n",
      "English:        you need help\n",
      "Spanish (true):  necesitas ayuda\n",
      "Spanish (pred):  \tecesntos oqiuo\n",
      "-\n",
      "English:        theres something out there\n",
      "Spanish (true):  hay algo ahi fuera\n",
      "Spanish (pred):  goq oryi\to hez e\t er co\te\n",
      "-\n",
      "English:        tom enjoys working here i think\n",
      "Spanish (true):  a tomas le gusta trabajar aqui creo\n",
      "Spanish (pred):  tad se se\ttna o ros uas q do\n",
      "-\n",
      "English:        you may park here\n",
      "Spanish (true):  puede estacionar aqui\n",
      "Spanish (pred):  pieues gobro\n",
      "-\n",
      "English:        he was my best friend\n",
      "Spanish (true):  era mi mejor amigo\n",
      "Spanish (pred):  er e\n",
      "-\n",
      "English:        im fine thank you\n",
      "Spanish (true):  estoy bien gracias\n",
      "Spanish (pred):  de estaq uesosnoua to\n",
      "-\n",
      "English:        she spoke rapidly\n",
      "Spanish (true):  ella hablo rapido\n",
      "Spanish (pred):  erro sobno f\n",
      "-\n",
      "English:        i tried to write him\n",
      "Spanish (true):  trate de escribirle\n",
      "Spanish (pred):  n\tte\tte so\n"
     ]
    }
   ],
   "source": [
    "# Take one sequence (part of the training set) for trying out decoding.\n",
    "for seq_index in range(2100, 2300):\n",
    "    input_seq = encoder_input_data_test[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    predicted_sentences.append(decoded_sentence[0:-1])\n",
    "    print('-')\n",
    "    print('English:       ', input_test_texts[seq_index])\n",
    "    print('Spanish (true): ', target_test_texts[seq_index][1:-1])\n",
    "    print('Spanish (pred): ', decoded_sentence[0:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vp77UjsvBtY_"
   },
   "source": [
    "### 5.3 Evaluate the BLEU score using the test set. (15 points)\n",
    "\n",
    "1. Use trained model above to calculate the BLEU score with testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "qzhMdhg7BtY_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\.conda\\envs\\GPU\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\abhis\\.conda\\envs\\GPU\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "target_list = [sentence.split() for sentence in target_test_texts[2100:2300]]\n",
    "predicted_list = [sentence.split() for sentence in predicted_sentences]\n",
    "\n",
    "smoothie = SmoothingFunction().method4\n",
    "references = target_list\n",
    "candidates = predicted_list\n",
    "\n",
    "score = corpus_bleu(references, candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "Vhj4aLhABtZA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8090178145048506e-155"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zzekjVSeBtZA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "8sRBPSs3BtY8",
    "vp77UjsvBtY_"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
