{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNjBFTn7-s8t"
   },
   "source": [
    "\n",
    "\n",
    "### Name: [Surya Giri]\n",
    "\n",
    "### Task: Change LSTM model to Bidirectional LSTM Modelï¼Œ translate English to target language and evaluate using Bleu score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-5Fzna2D6Pv"
   },
   "source": [
    "# 0. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rFw-l4vX-s8x"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Bidirectional, Concatenate, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YoenDeO-s8y"
   },
   "source": [
    "## 1. Data preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ey06SBdl-s8y"
   },
   "source": [
    "### 1.1. Load and clean text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Hb3qo4Up-s8y"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from unicodedata import normalize\n",
    "import numpy\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs\n",
    "\n",
    "def clean_data(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return numpy.array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EIbbLXzc-s80"
   },
   "outputs": [],
   "source": [
    "# e.g., filename = 'Data/deu.txt'\n",
    "filename = 'spa.txt'\n",
    "\n",
    "# e.g., n_train = 20000\n",
    "n_train = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NACrUlKR-s80"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "doc = load_doc(filename)\n",
    "\n",
    "# split into Language1-Language2 pairs\n",
    "pairs = to_pairs(doc)\n",
    "\n",
    "# clean sentences\n",
    "clean_pairs = clean_data(pairs)[0:n_train, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y8dyS1b9-s80",
    "outputId": "4c3d8ac6-5bfd-4833-f9a1-a9635dc4f485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youre here] => [estas aqui]\n",
      "[youre here] => [estais aqui]\n",
      "[youre late] => [estas retrasado]\n",
      "[youre lost] => [estas perdido]\n",
      "[youre mean] => [eres mala]\n",
      "[youre mean] => [eres mezquino]\n",
      "[youre mine] => [tu eres mio]\n",
      "[youre nice] => [eres simpatico]\n",
      "[youre nuts] => [estas loco]\n",
      "[youre nuts] => [estas chiflado]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3000, 3010):\n",
    "    print('[' + clean_pairs[i, 0] + '] => [' + clean_pairs[i, 1] + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Izg0goqQ-s81",
    "outputId": "b0d74774-b8e6-4e39-8151-ee7cb87286c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input_texts:  (20000,)\n",
      "Length of target_texts: (20000,)\n"
     ]
    }
   ],
   "source": [
    "input_texts = clean_pairs[:, 0]\n",
    "target_texts = ['\\t' + text + '\\n' for text in clean_pairs[:, 1]]\n",
    "\n",
    "print('Length of input_texts:  ' + str(input_texts.shape))\n",
    "print('Length of target_texts: ' + str(input_texts.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ivCo3kQ9-s82",
    "outputId": "5ed955ec-5956-4443-f002-dfb309e2e39f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of input  sentences: 18\n",
      "max length of target sentences: 55\n"
     ]
    }
   ],
   "source": [
    "max_encoder_seq_length = max(len(line) for line in input_texts)\n",
    "max_decoder_seq_length = max(len(line) for line in target_texts)\n",
    "\n",
    "print('max length of input  sentences: %d' % (max_encoder_seq_length))\n",
    "print('max length of target sentences: %d' % (max_decoder_seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I557udaO-s83"
   },
   "source": [
    "## 2. Text processing\n",
    "\n",
    "### 2.1. Convert texts to sequences\n",
    "\n",
    "- Input: A list of $n$ sentences (with max length $t$).\n",
    "- It is represented by a $n\\times t$ matrix after the tokenization and zero-padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rf17xvUA-s83",
    "outputId": "846672f9-0ec6-4e6e-e02a-6b5999534d82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoder_input_seq: (20000, 18)\n",
      "shape of input_token_index: 27\n",
      "shape of decoder_input_seq: (20000, 55)\n",
      "shape of target_token_index: 29\n"
     ]
    }
   ],
   "source": [
    "# encode and pad sequences\n",
    "def text2sequences(max_len, lines):\n",
    "    tokenizer = Tokenizer(char_level=True, filters='')\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    seqs = tokenizer.texts_to_sequences(lines)\n",
    "    seqs_pad = pad_sequences(seqs, maxlen=max_len, padding='post')\n",
    "    return seqs_pad, tokenizer.word_index\n",
    "\n",
    "\n",
    "encoder_input_seq, input_token_index = text2sequences(max_encoder_seq_length, \n",
    "                                                      input_texts)\n",
    "decoder_input_seq, target_token_index = text2sequences(max_decoder_seq_length, \n",
    "                                                       target_texts)\n",
    "\n",
    "print('shape of encoder_input_seq: ' + str(encoder_input_seq.shape))\n",
    "print('shape of input_token_index: ' + str(len(input_token_index)))\n",
    "print('shape of decoder_input_seq: ' + str(decoder_input_seq.shape))\n",
    "print('shape of target_token_index: ' + str(len(target_token_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qwiiHHCR-s84",
    "outputId": "cfb154b9-695f-44c9-c2d2-c515746cb5aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_encoder_tokens: 28\n",
      "num_decoder_tokens: 30\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens = len(input_token_index) + 1\n",
    "num_decoder_tokens = len(target_token_index) + 1\n",
    "\n",
    "print('num_encoder_tokens: ' + str(num_encoder_tokens))\n",
    "print('num_decoder_tokens: ' + str(num_decoder_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXvM2O9C-s84"
   },
   "source": [
    "**Remark:** To this end, the input language and target language texts are converted to 2 matrices. \n",
    "\n",
    "- Their number of rows are both n_train.\n",
    "- Their number of columns are respective max_encoder_seq_length and max_decoder_seq_length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOy6nDsB-s84"
   },
   "source": [
    "The followings print a sentence and its representation as a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "id": "XLFPi_3z-s85",
    "outputId": "3047ea87-46f7-4cfa-a8a2-12482a3c53bf"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\tno puede ser\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4JLul758-s85",
    "outputId": "a53b7ba6-7f8b-4e97-8861-ef6023513624"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  8,  3,  1, 17, 14,  2, 15,  2,  1,  5,  2, 10,  7,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_seq[100, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRDnyHB1-s85"
   },
   "source": [
    "## 2.2. One-hot encode\n",
    "\n",
    "- Input: A list of $n$ sentences (with max length $t$).\n",
    "- It is represented by a $n\\times t$ matrix after the tokenization and zero-padding.\n",
    "- It is represented by a $n\\times t \\times v$ tensor ($t$ is the number of unique chars) after the one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3SkZzeH-s86",
    "outputId": "39ae7e9a-ce14-4f09-c758-25eadb12de11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 18, 28)\n",
      "(20000, 55, 30)\n"
     ]
    }
   ],
   "source": [
    "# one hot encode target sequence\n",
    "def onehot_encode(sequences, max_len, vocab_size):\n",
    "    n = len(sequences)\n",
    "    data = numpy.zeros((n, max_len, vocab_size))\n",
    "    for i in range(n):\n",
    "        data[i, :, :] = to_categorical(sequences[i], num_classes=vocab_size)\n",
    "    return data\n",
    "\n",
    "encoder_input_data = onehot_encode(encoder_input_seq, max_encoder_seq_length, num_encoder_tokens)\n",
    "decoder_input_data = onehot_encode(decoder_input_seq, max_decoder_seq_length, num_decoder_tokens)\n",
    "\n",
    "decoder_target_seq = numpy.zeros(decoder_input_seq.shape)\n",
    "decoder_target_seq[:, 0:-1] = decoder_input_seq[:, 1:]\n",
    "decoder_target_data = onehot_encode(decoder_target_seq, \n",
    "                                    max_decoder_seq_length, \n",
    "                                    num_decoder_tokens)\n",
    "\n",
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8BMRatd-s86"
   },
   "source": [
    "## 3. Build the networks (for training)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKg1ptdt-s87"
   },
   "source": [
    "### 3.1. Encoder network\n",
    "\n",
    "- Input:  one-hot encode of the input language\n",
    "\n",
    "- Return: \n",
    "\n",
    "    -- output (all the hidden states   $h_1, \\cdots , h_t$) are always discarded\n",
    "    \n",
    "    -- the final hidden state  $h_t$\n",
    "    \n",
    "    -- the final conveyor belt $c_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "M2XBkPzl-s87"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM\n",
    "from keras.layers import Bidirectional, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "latent_dim = 150\n",
    "\n",
    "# inputs of the encoder network\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens), \n",
    "                       name='encoder_inputs')\n",
    "\n",
    "# set the LSTM layer\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True, \n",
    "                    dropout=0.5, name='encoder_lstm')\n",
    "_, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "\n",
    "\n",
    "encoder_bilstm = Bidirectional(LSTM(latent_dim, return_state=True, \n",
    "                                  dropout=0.5, name='encoder_lstm'))\n",
    "_, forward_h, forward_c, backward_h, backward_c = encoder_bilstm(encoder_inputs)\n",
    "\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "# build the encoder network model\n",
    "encoder_model = Model(inputs=encoder_inputs, \n",
    "                      outputs=[state_h, state_c],\n",
    "                      name='encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUY6TkXv-s87"
   },
   "source": [
    "Print a summary and save the encoder network structure to \"./encoder.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5GaLYb3o-s87",
    "outputId": "7416e776-11c9-4c89-fe79-df46dac9b5ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None, 28)]   0           []                               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  [(None, 300),        214800      ['encoder_inputs[0][0]']         \n",
      "                                 (None, 150),                                                     \n",
      "                                 (None, 150),                                                     \n",
      "                                 (None, 150),                                                     \n",
      "                                 (None, 150)]                                                     \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 300)          0           ['bidirectional[0][1]',          \n",
      "                                                                  'bidirectional[0][3]']          \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 300)          0           ['bidirectional[0][2]',          \n",
      "                                                                  'bidirectional[0][4]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 214,800\n",
      "Trainable params: 214,800\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "SVG(model_to_dot(encoder_model, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model(\n",
    "    model=encoder_model, show_shapes=False,\n",
    "    to_file='encoder.pdf'\n",
    ")\n",
    "\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUOZr3E8-s88"
   },
   "source": [
    "### 3.2. Decoder network\n",
    "\n",
    "- Inputs:  \n",
    "\n",
    "    -- one-hot encode of the target language\n",
    "    \n",
    "    -- The initial hidden state $h_t$ \n",
    "    \n",
    "    -- The initial conveyor belt $c_t$ \n",
    "\n",
    "- Return: \n",
    "\n",
    "    -- output (all the hidden states) $h_1, \\cdots , h_t$\n",
    "\n",
    "    -- the final hidden state  $h_t$ (discarded in the training and used in the prediction)\n",
    "    \n",
    "    -- the final conveyor belt $c_t$ (discarded in the training and used in the prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "wMDOGdfE-s88"
   },
   "outputs": [],
   "source": [
    "latent_dim = latent_dim * 2\n",
    "\n",
    "# inputs of the decoder network\n",
    "decoder_input_h = Input(shape=(latent_dim,), name='decoder_input_h')\n",
    "decoder_input_c = Input(shape=(latent_dim,), name='decoder_input_c')\n",
    "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
    "\n",
    "# set the LSTM layer\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, \n",
    "                    return_state=True, dropout=0.5, name='decoder_lstm')\n",
    "decoder_lstm_outputs, state_h, state_c = decoder_lstm(decoder_input_x, \n",
    "                                                      initial_state=[decoder_input_h, decoder_input_c])\n",
    "\n",
    "# set the dense layer\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
    "\n",
    "# build the decoder network model\n",
    "decoder_model = Model(inputs=[decoder_input_x, decoder_input_h, decoder_input_c],\n",
    "                      outputs=[decoder_outputs, state_h, state_c],\n",
    "                      name='decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEQmY_OO-s88"
   },
   "source": [
    "Print a summary and save the encoder network structure to \"./decoder.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BYG71GFx-s89",
    "outputId": "57a629fa-f1ca-4007-f6eb-a5998dd47270"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " decoder_input_x (InputLayer)   [(None, None, 30)]   0           []                               \n",
      "                                                                                                  \n",
      " decoder_input_h (InputLayer)   [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " decoder_input_c (InputLayer)   [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            [(None, None, 300),  397200      ['decoder_input_x[0][0]',        \n",
      "                                 (None, 300),                     'decoder_input_h[0][0]',        \n",
      "                                 (None, 300)]                     'decoder_input_c[0][0]']        \n",
      "                                                                                                  \n",
      " decoder_dense (Dense)          (None, None, 30)     9030        ['decoder_lstm[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 406,230\n",
      "Trainable params: 406,230\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "SVG(model_to_dot(decoder_model, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model(\n",
    "    model=decoder_model, show_shapes=False,\n",
    "    to_file='decoder.pdf'\n",
    ")\n",
    "\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRKiiS3m-s89"
   },
   "source": [
    "### 3.3. Connect the encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "LccIp3dm-s89"
   },
   "outputs": [],
   "source": [
    "# input layers\n",
    "encoder_input_x = Input(shape=(None, num_encoder_tokens), name='encoder_input_x')\n",
    "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
    "\n",
    "# connect encoder to decoder\n",
    "encoder_final_states = encoder_model([encoder_input_x])\n",
    "decoder_lstm_output, _, _ = decoder_lstm(decoder_input_x, initial_state=encoder_final_states)\n",
    "decoder_pred = decoder_dense(decoder_lstm_output)\n",
    "\n",
    "model = Model(inputs=[encoder_input_x, decoder_input_x], \n",
    "              outputs=decoder_pred, \n",
    "              name='model_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fLUGvaM-s8-",
    "outputId": "f4ebb5b9-f1ee-41f2-e9ba-d510d8a44159"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_training\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input_x (InputLayer)   [(None, None, 28)]   0           []                               \n",
      "                                                                                                  \n",
      " decoder_input_x (InputLayer)   [(None, None, 30)]   0           []                               \n",
      "                                                                                                  \n",
      " encoder (Functional)           [(None, 300),        214800      ['encoder_input_x[0][0]']        \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            [(None, None, 300),  397200      ['decoder_input_x[0][0]',        \n",
      "                                 (None, 300),                     'encoder[0][0]',                \n",
      "                                 (None, 300)]                     'encoder[0][1]']                \n",
      "                                                                                                  \n",
      " decoder_dense (Dense)          (None, None, 30)     9030        ['decoder_lstm[1][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 621,030\n",
      "Trainable params: 621,030\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model(\n",
    "    model=model, show_shapes=False,\n",
    "    to_file='model_training.pdf'\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdpexakP-s8-"
   },
   "source": [
    "### 3.4. Fit the model on the bilingual dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9CvSGthB-s8-",
    "outputId": "073c96cc-ed4f-4e6a-d8d2-5ef292ff9708"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoder_input_data(20000, 18, 28)\n",
      "shape of decoder_input_data(20000, 55, 30)\n",
      "shape of decoder_target_data(20000, 55, 30)\n"
     ]
    }
   ],
   "source": [
    "print('shape of encoder_input_data' + str(encoder_input_data.shape))\n",
    "print('shape of decoder_input_data' + str(decoder_input_data.shape))\n",
    "print('shape of decoder_target_data' + str(decoder_target_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R_3K5sIr-s8_",
    "outputId": "2b25a115-c615-4109-b049-9ea2c6ecb829"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "250/250 [==============================] - 13s 19ms/step - loss: 0.9157 - val_loss: 0.8342\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.7260 - val_loss: 0.7511\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.6885 - val_loss: 0.7044\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.6645 - val_loss: 0.6684\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.6442 - val_loss: 0.6435\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.6293 - val_loss: 0.6236\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.6167 - val_loss: 0.6062\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.6045 - val_loss: 0.5932\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.5953 - val_loss: 0.5816\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.5866 - val_loss: 0.5719\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.5770 - val_loss: 0.5616\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.5693 - val_loss: 0.5512\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.5628 - val_loss: 0.5426\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.5579 - val_loss: 0.5362\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.5510 - val_loss: 0.5299\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.5462 - val_loss: 0.5255\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.5420 - val_loss: 0.5210\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.5362 - val_loss: 0.5165\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.5302 - val_loss: 0.5104\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.5270 - val_loss: 0.5041\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.5228 - val_loss: 0.5008\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.5199 - val_loss: 0.4987\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.5157 - val_loss: 0.4938\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.5121 - val_loss: 0.4911\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.5087 - val_loss: 0.4883\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.5060 - val_loss: 0.4860\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.5020 - val_loss: 0.4823\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.4995 - val_loss: 0.4794\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.4963 - val_loss: 0.4760\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.4943 - val_loss: 0.4761\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.4916 - val_loss: 0.4733\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.4890 - val_loss: 0.4735\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.4853 - val_loss: 0.4726\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.4838 - val_loss: 0.4669\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.4803 - val_loss: 0.4625\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.4785 - val_loss: 0.4633\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.4755 - val_loss: 0.4595\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.4745 - val_loss: 0.4620\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.4713 - val_loss: 0.4591\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.4728 - val_loss: 0.4563\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.4671 - val_loss: 0.4576\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.4677 - val_loss: 0.4554\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.4653 - val_loss: 0.4524\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.4620 - val_loss: 0.4516\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.4606 - val_loss: 0.4509\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.4586 - val_loss: 0.4507\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.4568 - val_loss: 0.4473\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.4561 - val_loss: 0.4498\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.4541 - val_loss: 0.4440\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.4538 - val_loss: 0.4452\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data],  # training data\n",
    "          decoder_target_data,                       # labels (left shift of the target sequences)\n",
    "          batch_size=64, epochs=50, validation_split=0.2)\n",
    "\n",
    "model.save('seq2seq.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHPrj9IL-s8_"
   },
   "source": [
    "## 4. Make predictions\n",
    "\n",
    "\n",
    "\n",
    "### 4.1. Translate English to XXX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "7Pdeaipz-s8_"
   },
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "pDyi5q81-s9A"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "def decode_sequence(input_seq, temperature = 0.2):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # this line of code is greedy selection\n",
    "        # try to use multinomial sampling instead (with temperature)\n",
    "        # sampled_token_index = numpy.argmax(output_tokens[0, -1, :])\n",
    "\n",
    "        # Multinomial sampling with temperature\n",
    "        p = output_tokens[0, -1, :]\n",
    "        p = numpy.asarray(p).astype('float64')\n",
    "        p = p ** (1 / temperature)\n",
    "        p = p / numpy.sum(p)\n",
    "\n",
    "        next_onehot = numpy.random.multinomial(1,p,1)\n",
    "        sampled_token_index = numpy.argmax(next_onehot)\n",
    "\n",
    "        # To handle the 0 index error\n",
    "        if sampled_token_index == 0:\n",
    "          break\n",
    "        \n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pyAdLqpr-s9A",
    "outputId": "76731f93-eceb-4c44-f5a9-003aed9432cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "English:        hes skinny\n",
      "Spanish (true):  el esta delgado\n",
      "Spanish (pred):  er es i\t poca\n",
      "-\n",
      "English:        hes strong\n",
      "Spanish (true):  el es fuerte\n",
      "Spanish (pred):  es es o dn\ta\n",
      "-\n",
      "English:        hes stupid\n",
      "Spanish (true):  el es estupido\n",
      "Spanish (pred):  er esto o dnua\n",
      "-\n",
      "English:        hes stupid\n",
      "Spanish (true):  no le llega agua al tanque\n",
      "Spanish (pred):  er esto o dn\ta\n",
      "-\n",
      "English:        hes stupid\n",
      "Spanish (true):  es un salame\n",
      "Spanish (pred):  er esto o dn\to\n",
      "-\n",
      "English:        help me out\n",
      "Spanish (true):  ayudame\n",
      "Spanish (pred):  uejo es er core\n",
      "-\n",
      "English:        help me out\n",
      "Spanish (true):  ayudame a salir\n",
      "Spanish (pred):  er esto o do\n",
      "-\n",
      "English:        help me out\n",
      "Spanish (true):  echeme la mano\n",
      "Spanish (pred):  sn de dnsa\n",
      "-\n",
      "English:        help me out\n",
      "Spanish (true):  ayudame a salir\n",
      "Spanish (pred):  uejo esto co\n",
      "-\n",
      "English:        here i come\n",
      "Spanish (true):  aqui vengo\n",
      "Spanish (pred):  esto o do\n",
      "-\n",
      "English:        here i come\n",
      "Spanish (true):  ya estoy aqui\n",
      "Spanish (pred):  esto o do\n",
      "-\n",
      "English:        here she is\n",
      "Spanish (true):  aqui esta ella\n",
      "Spanish (pred):  estodas e\t ro paca\n",
      "-\n",
      "English:        here we are\n",
      "Spanish (true):  aqui estamos\n",
      "Spanish (pred):  er es o tad\n",
      "-\n",
      "English:        here we are\n",
      "Spanish (true):  aqui estamos\n",
      "Spanish (pred):  er o tad esto o do\n",
      "-\n",
      "English:        hi im tom\n",
      "Spanish (true):  hola soy tom\n",
      "Spanish (pred):  esto es o\to ue do\to\n",
      "-\n",
      "English:        hit it hard\n",
      "Spanish (true):  dele fuerte\n",
      "Spanish (pred):  este es dn dn\ta\n",
      "-\n",
      "English:        how are you\n",
      "Spanish (true):  como estas vos\n",
      "Spanish (pred):  cada esto tad\n",
      "-\n",
      "English:        how curious\n",
      "Spanish (true):  que curioso\n",
      "Spanish (pred):  cio\tta ca\tte\n",
      "-\n",
      "English:        how strange\n",
      "Spanish (true):  que raro\n",
      "Spanish (pred):  cada se sne\tta\n",
      "-\n",
      "English:        how strange\n",
      "Spanish (true):  que raro\n",
      "Spanish (pred):  cio\tta ca\tte\tto\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(2100, 2120):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('English:       ', input_texts[seq_index])\n",
    "    print('Spanish (true): ', target_texts[seq_index][1:-1])\n",
    "    print('Spanish (pred): ', decoded_sentence[0:-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17B-PF2h-s9A"
   },
   "source": [
    "### 4.2. Translate an English sentence to the target language\n",
    "\n",
    "1. Tokenization\n",
    "2. One-hot encode\n",
    "3. Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O7cunpmf-s9B",
    "outputId": "cbcd9eef-2842-4897-bd82-c30a38d6b92d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source sentence is: I love you\n",
      "translated sentence is: mmee  aammoo\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_sentence = 'I love you'\n",
    "\n",
    "tokenizer = Tokenizer(char_level=True, filters='')\n",
    "\n",
    "tokenizer.word_index = input_token_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences([input_sentence])\n",
    "\n",
    "input_sequence = pad_sequences(sequences, maxlen=max_encoder_seq_length, padding='post')\n",
    "\n",
    "input_x = onehot_encode(input_sequence,max_encoder_seq_length,num_encoder_tokens )\n",
    "\n",
    "translated_sentence = decode_sequence(input_x)\n",
    "\n",
    "print('source sentence is: ' + input_sentence)\n",
    "print('translated sentence is: ' + translated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrftQ1RQ-s9B"
   },
   "source": [
    "# 5. Evaluate the translation using BLEU score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKZOVr50-s9B"
   },
   "source": [
    "### 5.1. Partition the dataset to training, validation, and test. Build new token index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "4xK-mGO5-s9C"
   },
   "outputs": [],
   "source": [
    "clean_pairs = clean_data(pairs)[0:120000, :]\n",
    "input_texts = clean_pairs[:, 0]\n",
    "target_texts = ['\\t' + text + '\\n' for text in clean_pairs[:, 1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ilIzgb9b-s9C"
   },
   "outputs": [],
   "source": [
    "max_encoder_seq_length = max(len(line) for line in input_texts)\n",
    "max_decoder_seq_length = max(len(line) for line in target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "cF2cb128-s9C"
   },
   "outputs": [],
   "source": [
    "encoder_input_seq, input_token_index = text2sequences(max_encoder_seq_length, \n",
    "                                                      input_texts)\n",
    "decoder_input_seq, target_token_index = text2sequences(max_decoder_seq_length, \n",
    "                                                       target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Ki7d043C-s9C"
   },
   "outputs": [],
   "source": [
    "num_encoder_tokens = len(input_token_index) + 1\n",
    "num_decoder_tokens = len(target_token_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "s-as62pX-s9D"
   },
   "outputs": [],
   "source": [
    "encoder_input_data = onehot_encode(encoder_input_seq, max_encoder_seq_length, num_encoder_tokens)\n",
    "decoder_input_data = onehot_encode(decoder_input_seq, max_decoder_seq_length, num_decoder_tokens)\n",
    "\n",
    "decoder_target_seq = numpy.zeros(decoder_input_seq.shape)\n",
    "decoder_target_seq[:, 0:-1] = decoder_input_seq[:, 1:]\n",
    "decoder_target_data = onehot_encode(decoder_target_seq, \n",
    "                                    max_decoder_seq_length, \n",
    "                                    num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "siRtO9mZ-s9D",
    "outputId": "4d90aaf8-4fa9-40f4-9925-e31234c65138"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 87, 30)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "0w8kHVoJ-s9D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y5FovIBU-s9D",
    "outputId": "0fbd2e10-fcfd-460b-b420-aaebf9f5c9e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8X3WFoJp-s9E",
    "outputId": "a7e150d0-386e-48ef-cec8-58829e241d91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "hqiX-tjY-s9E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wGlXS1Od-s9E",
    "outputId": "06c49b95-236d-4afc-8ca6-928f7056d585"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_encoder_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DdNX9KZo-s9E",
    "outputId": "d003fafd-3cba-44f2-83f3-eff70a3d0254"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_decoder_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b4ttc2b-s9F",
    "outputId": "0e3a9d7e-201f-484c-ed9a-376eef1b29d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 44)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LB1DG2-D-s9F",
    "outputId": "de968a97-857b-4a0a-e41f-3d2a45abc57b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xj6A7UWH-s9F",
    "outputId": "92070dce-1ef6-4bb7-95a7-8863fbede4b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 87)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lH1YHbZI-s9G",
    "outputId": "8dc5830d-4c8f-42b0-b71f-8e9fc9fa551a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ElJkzMH-s9G",
    "outputId": "5051314b-d67e-48c5-db2e-15600f1d02a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_encoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yi2NJxg8-s9G",
    "outputId": "be16667a-6b79-4235-f05f-6eb36ddc5bd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "frpY-eE3-s9G",
    "outputId": "6b07a331-b5bb-4a07-90c3-a639fe62843b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 44, 28)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cXizqdpL-s9H",
    "outputId": "e20ab403-1459-4804-cc8f-4021a39475fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 87, 30)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VUCkwDo6-s9H",
    "outputId": "a4a6672f-552b-48ab-99d5-27e7c6911b2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 87, 30)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "0l5m01qE-s9H"
   },
   "outputs": [],
   "source": [
    "rand_indices = numpy.random.permutation(120000)\n",
    "train_indices = rand_indices[0:96000]\n",
    "valid_indices = rand_indices[96000:108000]\n",
    "test_indices = rand_indices[108000:120000]\n",
    "\n",
    "\n",
    "encoder_input_data_train = encoder_input_data[train_indices,:]\n",
    "encoder_input_data_valid = encoder_input_data[valid_indices,:]\n",
    "encoder_input_data_test = encoder_input_data[test_indices,:] \n",
    "\n",
    "decoder_input_data_train = decoder_input_data[train_indices,:]\n",
    "decoder_input_data_valid = decoder_input_data[valid_indices,:]\n",
    "decoder_input_data_test = decoder_input_data[test_indices,:] \n",
    "\n",
    "\n",
    "decoder_target_data_train = decoder_target_data[train_indices,:]\n",
    "decoder_target_data_valid = decoder_target_data[valid_indices,:]\n",
    "decoder_target_data_test = decoder_target_data[test_indices,:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhU4Ieey-s9I"
   },
   "source": [
    "### 5.2 Retrain your previous Bidirectional LSTM model with training and validation data and tune the parameters (learning rate, optimizer, etc) based on validation score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38qNVxQP-s9I",
    "outputId": "e2d0e5a1-0f1d-4d40-c4c1-b5ff2bd67471"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "188/188 [==============================] - 16s 62ms/step - loss: 0.9222 - val_loss: 0.7289\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - 10s 56ms/step - loss: 0.7885 - val_loss: 0.6790\n",
      "Epoch 3/20\n",
      "188/188 [==============================] - 10s 55ms/step - loss: 0.7644 - val_loss: 0.6499\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - 10s 55ms/step - loss: 0.7489 - val_loss: 0.6286\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - 10s 55ms/step - loss: 0.7366 - val_loss: 0.6108\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - 11s 56ms/step - loss: 0.7267 - val_loss: 0.5974\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - 10s 55ms/step - loss: 0.7185 - val_loss: 0.5862\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - 10s 55ms/step - loss: 0.7109 - val_loss: 0.5751\n",
      "Epoch 9/20\n",
      "188/188 [==============================] - 10s 56ms/step - loss: 0.7044 - val_loss: 0.5663\n",
      "Epoch 10/20\n",
      "188/188 [==============================] - 10s 55ms/step - loss: 0.6986 - val_loss: 0.5586\n",
      "Epoch 11/20\n",
      "188/188 [==============================] - 10s 55ms/step - loss: 0.6929 - val_loss: 0.5518\n",
      "Epoch 12/20\n",
      "188/188 [==============================] - 10s 55ms/step - loss: 0.6879 - val_loss: 0.5449\n",
      "Epoch 13/20\n",
      "188/188 [==============================] - 10s 55ms/step - loss: 0.6839 - val_loss: 0.5389\n",
      "Epoch 14/20\n",
      "188/188 [==============================] - 10s 55ms/step - loss: 0.6797 - val_loss: 0.5330\n",
      "Epoch 15/20\n",
      "188/188 [==============================] - 10s 55ms/step - loss: 0.6758 - val_loss: 0.5275\n",
      "Epoch 16/20\n",
      "188/188 [==============================] - 10s 55ms/step - loss: 0.6722 - val_loss: 0.5241\n",
      "Epoch 17/20\n",
      "188/188 [==============================] - 10s 55ms/step - loss: 0.6691 - val_loss: 0.5190\n",
      "Epoch 18/20\n",
      "188/188 [==============================] - 10s 55ms/step - loss: 0.6657 - val_loss: 0.5142\n",
      "Epoch 19/20\n",
      "188/188 [==============================] - 10s 55ms/step - loss: 0.6630 - val_loss: 0.5099\n",
      "Epoch 20/20\n",
      "188/188 [==============================] - 10s 55ms/step - loss: 0.6599 - val_loss: 0.5052\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adamax', loss='categorical_crossentropy')\n",
    "\n",
    "model.fit([encoder_input_data_train, decoder_input_data_train],  # training data\n",
    "          decoder_target_data_train,                       # labels (left shift of the target sequences)\n",
    "              batch_size=512, epochs=20, validation_data=([encoder_input_data_valid, decoder_input_data_valid], decoder_target_data_valid))\n",
    "\n",
    "model.save('seq2seq_tune.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "nfYgI2b1-s9I"
   },
   "outputs": [],
   "source": [
    "input_test_texts = input_texts[test_indices]\n",
    "target_texts = numpy.array(target_texts)\n",
    "target_test_texts = target_texts[test_indices]\n",
    "predicted_sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAb6oUAv-s9J",
    "outputId": "4b67c87f-53f6-4083-e53d-addb408591f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CDrKMLGO-s9J",
    "outputId": "3b7aaf6c-0953-4f57-93c4-3e1aea2b684a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p6NCAhwn-s9J",
    "outputId": "95a09dc4-d5b6-494a-ec49-80c1c19bff13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ew_3V2Mx-s9J",
    "outputId": "c1aad62b-b536-4048-e45d-5b168686fd29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "English:        i understand your point\n",
      "Spanish (true):  entiendo tu postura\n",
      "Spanish (pred):  se pe\tue vie es o o coso\n",
      "-\n",
      "English:        tom turned on the car radio\n",
      "Spanish (true):  tom encendio la radio del auto\n",
      "Spanish (pred):  tad se pista ue co\tto\tue e\t er co\tte\n",
      "-\n",
      "English:        i hope that youll help me\n",
      "Spanish (true):  espero que me ayuden\n",
      "Spanish (pred):  espe\n",
      "-\n",
      "English:        how did you ever get out of it\n",
      "Spanish (true):  como hizo usted para salir de aquello\n",
      "Spanish (pred):  cada te pieues o tad \n",
      "-\n",
      "English:        we should have gotten married\n",
      "Spanish (true):  debimos habernos casado\n",
      "Spanish (pred):  uebedas vie tad esto o dn\tna\to\n",
      "-\n",
      "English:        tom tipped the cab driver\n",
      "Spanish (true):  tom le dio propina al taxista\n",
      "Spanish (pred):  tad re unsa ue co\tto ue ro do\n",
      "-\n",
      "English:        my wife still hasnt met my parents\n",
      "Spanish (true):  mi esposa aun no conoce a mis padres\n",
      "Spanish (pred):  dn pou\n",
      "-\n",
      "English:        i do it once a year\n",
      "Spanish (true):  lo hago una vez al ano\n",
      "Spanish (pred):  \ta se pieue o ca\t e\t er dn\ta\n",
      "-\n",
      "English:        what kind of bird is this\n",
      "Spanish (true):  que tipo de pajaro es este\n",
      "Spanish (pred):  vie gocne esto e\t ro co\tto\n",
      "-\n",
      "English:        tom and mary played hideandgoseek\n",
      "Spanish (true):  tom y mary jugaron al esconderite\n",
      "Spanish (pred):  tad qado estobo po\n",
      "-\n",
      "English:        tom got hurt playing football\n",
      "Spanish (true):  tom se lastimo jugando al futbol americano\n",
      "Spanish (pred):  tad \ta paune o do\n",
      "-\n",
      "English:        tom would you leave us alone for a moment\n",
      "Spanish (true):  tom nos dejarias solos por un momento\n",
      "Spanish (pred):  tad q do\n",
      "-\n",
      "English:        im not in a position to accept it\n",
      "Spanish (true):  no estoy en condiciones de aceptar\n",
      "Spanish (pred):  \ta estaq seyi\n",
      "-\n",
      "English:        they made the goal\n",
      "Spanish (true):  ellos alcanzaron la meta\n",
      "Spanish (pred):  erras ca\t or pe\n",
      "-\n",
      "English:        an elderly person was resting under a tree\n",
      "Spanish (true):  un anciano reposaba bajo un arbol\n",
      "Spanish (pred):  o\to cas o\ttes esto o dicga\t e\t er po\n",
      "-\n",
      "English:        who have you helped lately\n",
      "Spanish (true):  a quien has ayudado ultimamente\n",
      "Spanish (pred):  vine\t vine\n",
      "-\n",
      "English:        she bought a chicken\n",
      "Spanish (true):  ella ha comprado un pollo\n",
      "Spanish (pred):  erro se se ca\ta e\t er cornua\n",
      "-\n",
      "English:        tom has never killed anybody\n",
      "Spanish (true):  tom no ha matado nunca a nadie\n",
      "Spanish (pred):  tad \ta estobo e\t ro co\to\n",
      "-\n",
      "English:        just so you know im an fbi agent\n",
      "Spanish (true):  entonces como sabes soy un agente del fbi\n",
      "Spanish (pred):  ca\ta vie es ta\t se\t o\t po\n",
      "-\n",
      "English:        stand by\n",
      "Spanish (true):  preparate\n",
      "Spanish (pred):  pa\n"
     ]
    }
   ],
   "source": [
    "# Take one sequence (part of the training set) for trying out decoding.\n",
    "for seq_index in range(2100, 2120):\n",
    "    input_seq = encoder_input_data_test[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    predicted_sentences.append(decoded_sentence[0:-1])\n",
    "    print('-')\n",
    "    print('English:       ', input_test_texts[seq_index])\n",
    "    print('Spanish (true): ', target_test_texts[seq_index][1:-1])\n",
    "    print('Spanish (pred): ', decoded_sentence[0:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQG45wJJ-s9K"
   },
   "source": [
    "### 5.3 Evaluate the BLEU score using the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pU4Cvb4D-s9K",
    "outputId": "af86cd77-b6b4-40ed-d714-de2b371cd11f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "target_list = [sentence.split() for sentence in target_test_texts[2100:2120]]\n",
    "predicted_list = [sentence.split() for sentence in predicted_sentences]\n",
    "\n",
    "smoothie = SmoothingFunction().method4\n",
    "references = target_list\n",
    "candidates = predicted_list\n",
    "\n",
    "score = corpus_bleu(references, candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8MtqZBof-s9K",
    "outputId": "2e7caa3c-3b80-4492-862c-a836d39a9d2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6443814082270685"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpf_YtaF-s9K"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Assignment3 (3)_10475010_Giri.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
